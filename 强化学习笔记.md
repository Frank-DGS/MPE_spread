# 0.预备知识

## Contraction mapping thorem（状态压缩定理）

fixed point：$x \in X$ 是fixed point of $f$ 当且仅当 $f:X \rightarrow X,f(x) = x$

contraction mapping(contraction function)：$f$ 是一个 contraction mapping 当 $||f(x_1) - f(x_2) \leq \gamma||x_1 - x_2||,\gamma \in (0,1)$，直观上来看就是经过这个函数后的两个点距离被压缩了，对于 $x \in R^n，||A|| \leq \gamma \leq 1$ 有 $||Ax_1 - Ax_2|| = ||A||||x_1 - x_2|| \leq \gamma||x_1 - x_2||$，所以 $f(x) = Ax$​ 是一个contraction mapping

Contraction mapping thorem：对于任意一个映射 $f$ 只要满足是一个 contraction mapping，则：

- 存在性：存在一个不动点 $x^*$
- 唯一性：不动点唯一存在
- 算法：对于数列 ${x_k}$ 其中 $x_{k+1} = f(x_k)$，则当 $k \rightarrow \infty$ 时有 $x_k \rightarrow x^*$ 且收敛速度极快（指数收敛）

证明略

## 算子

算子是函数到函数的映射，对于本文中常见的
$$
(T^\pi V)(s) = \underset{a}{\sum}\pi(a|s)[\underset{r}{\sum}p(r|s,a)r+\gamma \underset{s'}{\sum}p(s'|s,a)v_\pi(s')],\space \forall s \in \mathcal S
$$

相当于输入一个 $V(s)$ 可以得到一个新的 $V'(s)$ 函数，对于算子依然有状态压缩定理（目前对于算子的理解不是很深入，这是第一版写的内容，往后学习的过程发现这里更合适的理解是一个向量的函数，所以对于向量要定义一个范数，之后再统一），具体就上将上述的函数变为算子，标量变为函数，判断是否符合时只要找到一个范数保证完备（在这个范数含义下任意求极限极限都在空间中，sup 范数是最常用的）且可以证明 $\gamma < 1$​ 即可 。对于一个算子如果满足状态压缩定理，就可以利用迭代的方法去求解不动点，对于上面这个式子就可以求解出真实的 state value，注意利用这个这种方法求出的函数是一个数值表而非解析式


# 1.基本概念

**state**：agent 在环境中的状态，对于 grid-world 例子中就是当前的网格位置

**state space**：所有 state 构成的集合

**action**：对于每个 state 当前能采取的所有行动中的一个

**action space of a state**：对于当前状态，所有可能的行动

**state transition**：从一个状态采取某个行动跳转到了另一个状态，$s_1 \overset{a_1}{\rightarrow} s_2$，定义了环境的交互，也就是规则​

**forbidden area**：是一个被惩罚的状态，分为两种，一是可以进入但是会受到巨大惩罚，另一种是不能进入，第一种是更普遍且值得思考的

可以用一个表格来表示 state transition，但是只能表示确定性情况，如果某一状态进行相同行动可能进入不同状态则无法表示，更一般的表示方法是 state transition probability：$p(s_2|s_1,a_2)=x$

**policy**：告诉agent在当前状态下应该采取什么行动，依然使用条件概率表示，用 $\pi$ 表示策略，即 $\pi(a_1|s_1) = x_1$，表示 $s_1$ 状态下进行 $a_1$ 行动的概率是 $x_1$，可以用表格表示，实际实践时也是通过矩阵表示策略，选取策略时就是从 $0-1$ 均匀采样，不同的区间对应不同策略

**reward**：一个标量，在 agent 采取一个行动之后获得，正数代表鼓励这个行为，负数代表不希望，对其惩罚。特殊的，如果 reward 为 $0$ 一般表示不惩罚，还是相对偏鼓励的，另外正数也可以表示惩罚，负数代表鼓励（数学技巧，本质是一样的）。reward 可以被理解为人类与 agent 交互的手段，来引导 agent 产生正确的策略。和 state transition 一样，对于确定情况可以用表格表示，否则利用条件概率。注意reward依赖于当前状态，行动，下一状态三个因素，而写成  $p(r=r_1|s_1,a_1)$ 看似只考虑两个，实则是把下一状态相同的 reward 的取值合并，用 $r$ 的取值表示了。

**trajectory**：是一个state-action-reward链，$s_1 \xrightarrow[r=0]{a_2}s_2 \xrightarrow[r=0]{a_3}a_5$​

**return**：针对 trajectory 而言的，等于整条轨迹reward的总和。可以用 return 来比较两个 trajectory 的好坏，反映背后策略的优劣

一个 trajectory 可能是无限长的（没有终止条件），此时 return 会发散无意义，所以引入了 discount rate（折旧因子），加入折旧因子的 return 就是 discount return：
$$
\text{discount return} = \gamma^0 r_1 + \gamma^1 r_2 +\gamma^2 r_3 + \dots , \quad \gamma \in [0, 1)
$$
将其转化为了收敛的，并平衡了远近的奖励，较小的 $\gamma$​​ 衰减的较快，更加近视，相反则更加长远

**terminal state**：终止状态，agent 到此会终止

**episode**：一个到达终止状态停了的 trajectory 就是一个 episode(trial)，表示一次尝试，一个 episode 一般是有限步的，这样的任务也称为 episodic tasks

有些任务是没有 terminal state 的，这意味着agent与环境的交互要一直持续下去，这样的任务称为 continuing tasks，不一定都是一定无限，持续很长时间也算，不需要区分这两种方法，因为可以将 episodic tasks 转化为 continuing tasks，两种方法：

1. 将target state设置为absorbing state，一旦agent进入到这个状态就不会离开了，并且 $r=0$
2. 将target state设置为一个普通状态，依然可以进行后续转移

一般使用第二种，更有一般性

以上就是强化学习中的基本概念，下面给出更数学化的定义，Markov decision process(MDP) 马尔科夫决策过程，是强化学习中用于建模序贯决策问题（指需要在多个时间步中连续做出决策的问题）的标准数学框架，用于描述agent与环境的交互过程：

- state：状态的集合 $\mathcal S$
- action：行动的集合 $\mathcal A(s)$​
- reward：奖励的集合 $\mathcal R(s,a)$，这是一个期望
- state transition probability：$p(s'|s,a)$
- reward probability：$p(r|s,a)$
- policy：$\pi(a|s)$

这个框架（假设）具有Markov 性质，即：
$$
p(s_{t+1}|a_{t+1},s_t,\dots,s_1)=p(s_{t+1}|a_{t+1},s_t) \\
r(s_{t+1}|a_{t+1},s_t,\dots,s_1)=r(s_{t+1}|a_{t+1},s_t)
$$

# 2.贝尔曼公式

return 可以反映策略的好坏，去评估策略，一种求解方法是 boostrapping（自举）：

![image-20250805031408181](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250805031408181.png)

对于这样一种情况，设 $v_i$ 为从 $s_i$ 出发的 return，即：
$$
v_1 = r_1+ \gamma v_2 \\
v_2 = r_2+ \gamma v_3 \\
v_3 = r_3+ \gamma v_4 \\
v_4 = r_4+ \gamma v_1
$$
写作矩阵形式：
$$
\underbrace{\begin{bmatrix}v_1\\v_2\\v_3\\v_4\end{bmatrix}}_{v} = 
\begin{bmatrix}r_1\\r_2\\r_3\\r_4\end{bmatrix} +
\begin{bmatrix}\gamma v_2\\\gamma v_3\\\gamma v_4\\\gamma v_1\end{bmatrix} =
\underbrace{\begin{bmatrix}r_1\\r_2\\r_3\\r_4\end{bmatrix}}_{r} + \gamma
\underbrace{\begin{bmatrix}0&1&0&0\\0&0&1&0\\0&0&0&1\\1&0&0&0\end{bmatrix}}_{P}
\underbrace{\begin{bmatrix}v_1\\v_2\\v_3\\v_4\end{bmatrix}}_{v}\\
v = r + \gamma P v
$$
这就是贝尔曼公式（不过是针对上例情况的贝尔曼公式，后续介绍更一般的）

![image-20250806145714402](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250806145714402.png)

注意这里说明了 $S,A,R$ 是随机变量，这点很重要，因为动作的选择以及状态的跳转是有随机性的（故奖励也一样），单步的过程持续下去得到多步的 trajectory，并获得 discounted return $G_t$ 的表达式，同样也是一个随机变量，而 state value 就是 $G_t$ 的期望：
$$
v_\pi(s) = \mathbb E[G_t|S_t]
$$
state value 的全称是 state value function，它是一个关于 $s$ 的方程，表示的是在策略 $\pi$ 下从状态 $s$ 出发，所能获得的折扣回报 $G_t$ 的期望值，所以它是一个条件期望，根据出发状态的 $s$ 变化而变化，这个方程是基于策略 $\pi$ 的。state value 代表当前状态的价值，在同一环境下可以比较策略的优劣

return 是针对单个 trajectory 的回报，而 state value 是从某一状态出发所有 trajectory 的均值

贝尔曼公式描述的是不同状态的 state value 之间的关系，用来计算 state value：
$$
\begin{align*}
G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \\
&= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\
&= R_{t+1} + \gamma G_{t+1}
\end{align*}
$$
$$
\begin{align*}
v_\pi(s) &= \mathbb E[G_t|S_t = s] \\
&= \mathbb E[R_{t+1}+\gamma G_{t+1}|S_t = s] \\
&= \mathbb E[R_{t+1}|S_t = s] +\mathbb E[\gamma G_{t+1}|S_t = s]
\end{align*}
$$

先考虑第一项，即从当前状态跳转到下一状态的期望，immediate rewards：
$$
\begin{align*}
\mathbb E[R_{t+1}|S_t = s] &= \underset{a}{\sum}\pi(a|s)\mathbb E[R_{t}|S_t = s] \\
&= \underset{a}{\sum}\pi(a|s) \underset{r}{\sum}p(r|s,a)r
\end{align*}
$$
然后是第二项，下一时刻的 state value：
$$
\begin{align*}
\mathbb E[G_{t+1}|S_t = s] &= \underset{s'}{\sum}\mathbb E[G_{t+1}|S_t = s,S_{t+1}=s']p(s'|s) \\
&=  \underset{s'}{\sum}\mathbb E[G_{t+1}|S_{t+1}=s']p(s'|s)\space\text{根据马尔科夫性质} \\
&= \underset{s'}{\sum}v_\pi(s')p(s'|s) \\
&= \underset{s'}{\sum}v_\pi(s')\underset{a}{\sum}p(s'|s,a)\pi(a|s) \\
&= \underset{s'}{\sum}\underset{a}{\sum}p(s'|s,a)\pi(a|s)v_\pi(s') \\
&= \underset{a}{\sum}\underset{s'}{\sum}p(s'|s,a)\pi(a|s)v_\pi(s') \\
&= \underset{a}{\sum}\pi(a|s)\underset{s'}{\sum}p(s'|s,a)v_\pi(s') \\
\end{align*}
$$
两式结合：
$$
v_\pi(s) = \underset{a}{\sum}\pi(a|s)[\underset{r}{\sum}p(r|s,a)r+\gamma \underset{s'}{\sum}p(s'|s,a)v_\pi(s')],\space \forall s \in \mathcal S
$$
这就是贝尔曼公式，建立了 $s,s'$​ 的 state value 之间的关系，非常通用的一个公式，仅依赖于 policy($\pi(a|s)$) 和 environment model($p(r|s,a),p(s'|s,a)$)，没有其他限制，后续讨论也分为模型已知和未知两种情况（model-based，model-free）

state value 是通过 bootstrapping 计算的，对于环境已知的情况，将每个状态代入到贝尔曼公式并转化为 Matrix-vector 形式即可求解：
$$
\begin{align*}
v_\pi(s) &= \underset{a}{\sum}\pi(a|s)[\underset{r}{\sum}p(r|s,a)r+\gamma \underset{s'}{\sum}p(s'|s,a)v_\pi(s')] \\
&= r_\pi(s) + \gamma \underset{s'}{\sum}p_\pi(s'|s)v_\pi(s')
\end{align*}
$$

$$
\text{其中}\space r_\pi(s)\overset{\triangle}{=}\underset{a}{\sum}\pi(a|s)\underset{r}{\sum}p(r|s,a)r, \\
p_\pi(s'|s)\stackrel{\triangle}{=}\underset{a}{\sum}\pi(a|s)p(s'|s,a)
$$

前者是 immediate reward 的平均值，第二项是 $s$ 转移到 $s'$ 的概率，即可组合为向量：
$$
v_\pi = r_\pi + \gamma P_\pi v_\pi \\
$$

$$
\begin{align*}
\text{其中}\space v_\pi &= [v_\pi(s_1),\dots,v_\pi(s_n)]^T \\
r_\pi &= [r_\pi(s_1),\dots,r_\pi(s_n)]^T \\
P_\pi &\in \mathbb R^{n \times n},[P_\pi]_{i,j} = p_\pi(s_j|s_i)
\end{align*}
$$

$P_\pi$​ 是 state transition 矩阵

**policy evaluation**：指给出一个策略，并求解 state value 的过程

下面给出两种贝尔曼公式的求解方法：

1. closed-form solution（闭合解），$v_\pi = (I-\gamma P_\pi)^{-1}r_\pi$ ，对于 $I-\gamma P_\pi$ 一定是可逆的，具体证明略。优美但复杂度高（矩阵求解复杂度近似 $O(n^3)$​）
2. iterative solution（迭代解），对于当前算子满足状态压缩定理，可以利用迭代求解不动点得到真实的state value，$v_{k+1} = r_\pi + \gamma P_\pi v_k$​，直到收敛

在 sup 范数下对于算子满足状态压缩定理的证明：
$$
||V||_\infin = \text{sup}_s||V(s)||,\text{即所有状态取值的绝对值的上确界} \\
\text{对于 }r_\pi + \gamma P_\pi v_\pi\text{ 写作算子形式即为：} \\
(T^{\pi}V)(s) = \underset{a}{\sum}\pi(a|s)[\underset{r}{\sum}p(r|s,a)r+\gamma \underset{s'}{\sum}p(s'|s,a)v_\pi(s')]
$$

$$
\begin{align*}
||T^\pi V_1 - T^\pi V_2||_\infin &= \underset{s}{\text{sup}}|\underset{a}{\sum}\pi(a|s)\gamma \underset{s'}{\sum}p(s'|s,a)(V_1(s') - V_2(s'))| \\
&= \underset{s}{\text{sup}}\underset{a}{\sum}\pi(a|s)\gamma \underset{s'}{\sum}p(s'|s,a)|V_1(s') - V_2(s')| \\
&\leq \underset{s}{\text{sup}}\underbrace{\underset{a}{\sum}\pi(a|s)}_1\gamma \underbrace{\underset{s'}{\sum}p(s'|s,a)}_1\underset{s''}{\text{sup}}|V_1(s'') - V_2(s'')| \\
&= \gamma ||V_1 - V_2||_\infin,\text{ 得证}
\end{align*}
$$

**action value**：state value 是从一个状态开始得到的 return 的期望值，而 action return 是从一个状态开始并选取一个动作得到的 return 的期望着，相当于多固定了一个初始动作，action value 越大说明在这个状态下选取这个动作更加合理，给出定义：
$$
q_\pi(s,a) = \mathbb E[G_t|S_t = s, A_t = t]
$$
故有：
$$
\underbrace{\mathbb E [G_t|S_t = s]}_{v_\pi(s)} = \underset{a}{\sum}\underbrace{\mathbb E[G_t|S_t = s, A_t = t]}_{q_\pi(s,a)}\pi (a|s) \\
\text{与 }v_\pi(s) = \underset{a}{\sum}\pi(a|s)[\underset{r}{\sum}p(r|s,a)r+\gamma \underset{s'}{\sum}p(s'|s,a)v_\pi(s')]\text{ 对比可得} \\
q_\pi(s,a) = \underset{r}{\sum}p(r|s,a)r+\gamma \underset{s'}{\sum}p(s'|s,a)v_\pi(s')
$$
注意即使当前状态根据策略有些动作无法执行，但不代表这个 action value 就是 $0$ 或者无法计算，根据上式依然可以计算，这点可以帮助制定更好的策略，当前状态无法执行的策略只是 $\pi(a|s) = 0$，思考公式发现，前一部分就是当前状态选当前策略的 immediate reward 的期望，后者是当前状态选当前策略下一个状态的 state value 的期望，再乘以一个折旧因子，因为是下一步骤了，和 action value 本身的定义一致（想说明虽然这个是公式推导到的，但是逻辑上也很通顺）

# 3.最优策略及贝尔曼最优公式

action value 的作用就是评估当前状态采取不同动作的优劣，并对策略进行调整，一种最简单的方法就是改为当前状态一定执行 action value 最高的动作，为什么可以根据 action value 去改呢，直观上这个东西本身就是评价动作的好坏，从数学上并不容易解释，因为这个调整是一个贪心的，当前的状态选择是依赖于全局的策略的，当其他部分的策略不怎么优时，这种方法对当前状态得到的策略并不是全局最优时当前状态的策略，某个状态改变时其他状态（甚至自身）的 action value 都会发生变化，这里先给一个结论：对所有状态依次调整（具体怎么调整还没说，应该不是上面这种最简单的），不断循环，最终会收敛于最优策略。本节会对此证明（通过贝尔曼最优公式）

之前说过 state value 可以衡量一个策略的好坏，现在给出直接的定义：
$$
\text{if } v_{\pi_1}(s) \geq v_{\pi_2}(s), \space \forall s \in \mathcal S \\
\text{then } \pi_1 \text{ is better than } \pi_2
$$
下面给出最优策略的定义：
$$
v_{\pi^*}(s) \geq v_{\pi}(s), \space \forall s \in \mathcal S, \forall \pi \in \Pi
$$
这个策略非常理想，要求在所有状态上均比所有策略都优，这里会有疑问：

1. 最优策略存在吗？
2. 最优策略唯一吗？
3. 最优策略是确定性还是非确定性？
4. 如何得到？

通过贝尔曼最优公式可以回答上述所有问题

贝尔曼最优公式：
$$
\begin{align*}
v(s) &= \underset{\pi}{max} \underset{a}{\sum}\pi(a|s)[\underset{r}{\sum}p(r|s,a)r+\gamma \underset{s'}{\sum}p(s'|s,a)v_\pi(s')],\space \forall s \in \mathcal S \\
&= \underset{\pi}{max} \underset{a}{\sum}\pi(a|s) q(s,a)
\end{align*}
$$
如果真有 $v$ 能使上式成立，那此时对应的策略就是最优策略。相比于贝尔曼公式就是加了一个最值，变成了一个优化问题，$p(r,|s,a),p(s'|s,a)$ 是已知的，$v,\pi$ 是要求的。写成矩阵形式与之前一致：
$$
v = \underset{\pi}{max}(r_\pi + \gamma P_\pi v)
$$
注意这里的 $\underset{\pi}{max}$ 是按元素执行的，两边的 $v$ 只有在逐行比较的意义下耦合，左边的 $v$ 是固定但未知的向量，每个分量 $v(s)$ 已经是对所有策略取完最大值后的最优值，右边的 $v$ 是根据每个候选策略 $\pi$ 算出的。所以这只是一个表示形式，真正求解还要逐行。

对于每一行即确定的状态 $s$：
$$
v(s) = \underset{\pi}{max} \underset{a}{\sum}\pi(a|s)[\underset{r}{\sum}p(r|s,a)r+\gamma \underset{s'}{\sum}p(s'|s,a)v_\pi(s')]
$$
对于 $v = \underset{\pi}{max}(r_\pi + \gamma P_\pi v)$，可以写作 $f(v) := \underset{\pi}{max}(r_\pi + \gamma P_\pi v)$ ，如果证明 $f$ 为 contraction mapping，则可以利用迭代求解不动点得到 $v^*$​，下面进行证明：

假设 $\pi_i^* = \underset{\pi}{\text{argmax}}(r_\pi + \gamma P_\pi v_i),\pi_i^*$ 在这里是指策略的集合，并由上述已证明一定是确定性策略，这里的策略也作此限制
$$
f(v_1) = \underset{\pi}{max}(r_\pi + \gamma P_\pi v_1) = r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 \geq r_{\pi_2^*} + \gamma P_{\pi_2^*} v_1\\
f(v_2) = \underset{\pi}{max}(r_\pi + \gamma P_\pi v_2) = r_{\pi_2^*} + \gamma P_{\pi_2^*} v_2 \geq r_{\pi_1^*} + \gamma P_{\pi_1^*} v_2
$$

注意这里的 $\geq$ 是逐行均满足的
$$
\begin{align*}
f(v_1) - f(v_2) &= r_{\pi_1^*} + \gamma P_{\pi_1^*}v_1 - (r_{\pi_2^*} + \gamma P_{\pi_2^*}v_2) \\
&\leq r_{\pi_1^*} + \gamma P_{\pi_1^*}v_1 - (r_{\pi_1^*} + \gamma P_{\pi_1^*}v_2) \\
&= \gamma P_{\pi_1^*}(v_1 - v_2) \\
\end{align*}
$$

$$
\text{同理 }f(v_2) - f(v_1) \leq \gamma P_{\pi_2^*}(v_2 - v_1) \\
\text{令 }z = max\{|\gamma P_{\pi_1^*}(v_1 - v_2)|,|\gamma P_{\pi_2^*}(v_1 - v_2)|\} \in \mathbb R^{|\mathcal S|}\\
$$

这里的 $max(\cdot),max\{\cdot\},|\cdot|,\leq,\geq$ 都是逐元素操作的，故 $z \geq 0$ 并有：
$$
-z \leq \gamma P_{\pi_2^*}(v_1 - v_2) \leq f(v_1) - f(v_1) \leq \gamma P_{\pi_1^*}(v_1 - v_2) \leq z \\
||f(v_1) - f(v_1)|| \leq z \\
\text{进而 }||f(v_1) - f(v_1)||_\infin \leq ||z||_\infin
$$
令 $z_i$ 为 $z$ 的第 $i$ 项，$p_i^T,q_i^T$ 分别为 $P_{\pi_1^*},P_{\pi_2^*}$ 的第 $i$ 行：
$$
z_i = max\{\gamma|p_i^T(v_1 - v_2)|,\gamma|q_i^T(v_1 - v_2)\} \\
|p_i^T(v_1 - v_2)| \leq p_i^T|v_1 - v_2| \leq ||v_1 - v_2||_\infin \\
\text{同理 } |q_i^T(v_1 - v_2)| \leq ||v_1 - v_2||_\infin \\
||z||_\infin = \underset{i}{max}|z_i| \leq \gamma ||v_1 - v_2||_\infin \\
||f(v_1) - f(v_2)||_\infin \leq \gamma ||v_1 - v_2||_\infin
$$
得证，同时可以证明最优策略一定存在，不过不一定唯一，是确定性的（也可以不是比如某个状态执行哪个动作都一样那可以随机分配），可以迭代求不动点得到，回答了上述四个问题。

使用迭代求解，在每轮求解 $f(v)$ 时观察（注意只考虑这个函数，而不管左侧最终要等于 $v^*$ 这件事）每行的值只与当前行对应状态的策略有关，所以每行对于策略的选择只用考虑当前状态即可：
$$
\text{令 }a^*(s) = \underset{a}{\text{argmax }} q(a,s), \forall s \in \mathcal S \\
 q(a,s) = \underset{r}{\sum}p(r|s,a)r+\gamma \underset{s'}{\sum}p(s'|s,a)v_\pi(s')
$$

注意这里的 $q(a,s)$ 并不是真实含义的 action value，毕竟 $v$ 也并不是真正的 state value，这里只是将 $v$ 代入到真实的 $v$ 求解真实的 $q$ 的公式中，只有等式两边成立时它们在具备真实的含义，注意分辨。对于计算 $f$ 时，$\pi$ 与 $v$ 不再有关系，它们的关系是在 $v$ 是真实的 state value 时才有的，在证明它符合状态压缩定理的时候也是不存在这两者之间的联系的，它们就是两个无关的变量，即 $v$ 就是一个 $\mathbb R^{|\mathcal S|}$ 向量，$\pi$ 就是任意一个策略（只要保证每个状态下，所有动作的概率和为 $1$，对于每个动作的概率都 $\in [0,1]$ 即可）（这一点是绝对数学上严谨的，如果之后看到遗忘了多思考一下等式和 $f$ 函数的含义区别，不要质疑），所以求解 $f(v)$ 去计算新的 $v'$ 值时也是分为两步，（1）选取当前的最优 $\pi$，（2）计算 $v'$，现在的唯一为解决的事情就是如何找到对于 $v$ 能使 $f(v)$ 最大的 $\pi$ 了，下面考虑对于所有策略 $\pi$ 都有：
$$
\begin{align*}
&\underset{a}{\sum}\pi(a|s)[\underset{r}{\sum}p(r|s,a)r+\gamma \underset{s'}{\sum}p(s'|s,a)v_\pi(s')] \\
\leq &\underset{a}{\sum}\pi(a|s)[\underset{r}{\sum}p(r|s,a^*(s))r+\gamma \underset{s'}{\sum}p(s'|s,a^*(s))v_\pi(s')] \\
= &\underset{r}{\sum}p(r|s,a^*(s))r+\gamma \underset{s'}{\sum}p(s'|s,a^*(s))v_\pi(s') \\
= &\space q(a^*(s),s)
\end{align*}
$$


于是每次迭代就是求出对于此时 $v$ 每个状态的最大 $q(a,s)$，并将该状态的策略改为一定执行这个 $a^*(s)$，得到此时最佳的 $\pi$，然后就可以求解得到新的 $v$，并进行下一次迭代，直到 $||v_{k+1}-v_k||$ 小于设定的阈值。

对于模型参数的选择，可以改变的是 $\gamma$ 与 $r$

当 $\gamma$ 较小时，后续的动作影响很小，模型比较近视，会更贪心只考虑当下，极端的就是 $\gamma = 0$ 时就会在当前可选动作的即时奖励最大的里面选一个，而 $\gamma$ 较大时就会更远视，可以考虑更多可能并打破常规（会尝试走进 forbidden area 以求总体的价值更大）

对于修改 $r \rightarrow c*r+d\quad c,d \in \mathbb R,c > 0$，对于任意策略有：
$$
\begin{align*}
v'_\pi &= c*r_\pi + d1 + \gamma P_\pi v'_\pi \\
v'_\pi &= (I-\gamma P_\pi)^{-1}(c*r_\pi + d1) \\
v_\pi &= (I-\gamma P_\pi)^{-1}r_\pi \\
v'_\pi &= c*v_\pi + (I-\gamma P_\pi)^{-1}d1 \\
\text{将其}&\text{展开为 Neumann 级数，或者特征值方法：} \\
v'_\pi &= c*v_\pi + \frac{d}{1-\gamma}1 \\
\end{align*}
$$
是一个增的变化，所以最优策略不会发生变化

# 4.值迭代与策略迭代

## 一、值迭代

相当于又将 $3$ 中的流程说了一遍，重点可以看之前的表述
$$
v_{k+1} = f(v_k) = \underset{\pi}{max}(r_\pi + \gamma P_\pi v_k),k=1,2,3\dots
$$

具体的求解分为两步：

1. policy update：$\pi_{k+1} = \underset{\pi}{\text{argmax}}(r_\pi + \gamma P_\pi v_k)$​
2. value update：$v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_k$

流程：

![image-20250821222856139](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250821222856139.png)

## 二、策略迭代

先给出一个初始策略 $\pi_0$，然后进行迭代，每次迭代分两步：

1. policy evaluation：利用贝尔曼公式去迭代得到当前策略下的 $v_{\pi_k}$：
   $$
   v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}
   $$
   就是在评估这个策略

2. policy improvement：求得对于当前 $v_k$ “最好”的策略（其实就是求让 $f(v_k)$ 最大的 $\pi$）：
   $$
   \pi_{k+1} = \underset{\pi}{\text{argmax}}(r_{\pi} + \gamma P_{\pi}v_{\pi_k})
   $$

对于步骤 $1$ 很明确，对于 $2$ 求解很容易，和之前一样，每个状态确定性执行 $q(a,s)$ 最大的动作（这里的 $q$ 也不是真实含义的 action value）。但是有几个问题需要解答：

1. 为什么 PI 之后 $\pi_{k+1}$ 要优于 $\pi_k$，这里的优没有明确一个定义，大致理解就是新的策略更好
2. 为什么这么迭代最后能找到最优策略 $\pi^*$
3. 值迭代与策略迭代有什么关系

对于问题 $1$，这里的依据是策略迭代定理：
$$
if \space \pi_{k+1} = \underset{\pi}{\text{argmax}}(r_{\pi} + \gamma P_{\pi}v_{\pi_k}),\text{then } v_{\pi_{k+1}} \geq v_{\pi_k},\forall k \in N^*
$$
这里 $\geq$ 和之前的一样，都是表示对于每个状态都要保证，下面给出证明：
$$
v_{\pi_{k+1}} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}} \\
v_{\pi_{k}} = r_{\pi_{k}} + \gamma P_{\pi_{k}} v_{\pi_{k}} \\
r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k}} \geq r_{\pi_{k}} + \gamma P_{\pi_{k}} v_{\pi_{k}} \\
$$

$$
\begin{align*}
v_{\pi_{k}} - v_{\pi_{k+1}} &= (r_{\pi_{k}} + \gamma P_{\pi_{k}} v_{\pi_{k}}) - (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}) \\
&\leq (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k}}) - (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}) \\
&= \gamma P_{\pi_{k+1}} (v_{\pi_{k}} - v_{\pi_{k+1}})
\end{align*}
$$

这一过程可以无限迭代：
$$
v_{\pi_{k}} - v_{\pi_{k+1}} \leq \gamma P_{\pi_{k+1}} (v_{\pi_{k}} - v_{\pi_{k+1}}) \leq \gamma P_{\pi_{k+1}} (\gamma P_{\pi_{k+1}} (v_{\pi_{k}} - v_{\pi_{k+1}})) \\
=\gamma^2 P_{\pi_{k+1}}^2 (v_{\pi_{k}} - v_{\pi_{k+1}}) \leq \dots \leq \underset{n\rightarrow\infin}{\text{lim}}\gamma^n P_{\pi_{k+1}}^n(v_{\pi_{k}} - v_{\pi_{k+1}})
$$
由于 $P_{\pi_{k+1}}$ 是行随机矩阵，所以 $P_{\pi_{k+1}} ^n$ 依然是（简单思考一下计算过程即可证明），又因为 $\gamma \in (0,1)$，上述的极限为 $0$（因为 $P_{\pi_{k+1}}^n$ 是行随机矩阵，所以有界，有界乘零），有 $v_{\pi_{k}} - v_{\pi_{k+1}} \leq 0$，得证，并且将这个“优”做了明确，即所有状态的 state value 均大于等于先前。

对于问题 $2$，通过 $1$ 虽然可以知道策略一定是越来越好的，但是还是不知道是否一定能到达 $v^*$，下面给出证明：

要证明的是 $\{v_{\pi_k}\}_{k=0}^\infin = v^*$，先引入值迭代的序列，我们知道 $\{v_{k}\}_{k=0}^\infin = v^*$，其中 $v_{k+1} = f(v_k)$

对于 $k = 0$ 对于任意 $\pi_0$ 总可以找到一个 $v_0$ 使得 $v_{\pi_0} \geq v_0$，利用数学归纳法，假设 $k \geq 0$ 时都有 $v_{\pi_k} \geq v_k$，对于 $k+1$：
$$
\begin{align*}
v_{\pi_{k+1}} - v_{k+1} &= (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}) - \underset{\pi}{\text{max}}(r_\pi + \gamma P_\pi v_k) \\
&\geq (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k}}) - \underset{\pi}{\text{max}}(r_\pi + \gamma P_\pi v_k) \\
&= (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k}}) - (r_{\pi_k'} + \gamma P_{\pi_k'} v_k) \\
& \pi_k' = \underset{\pi}{\text{argmax}}(r_\pi + \gamma P_\pi v_k) \\
&\geq (r_{\pi_k'} + \gamma P_{\pi_k'} v_{\pi_{k}}) - (r_{\pi_k'} + \gamma P_{\pi_k'} v_k) \\
&= \gamma P_{\pi_k'}(v_{\pi_k'} - v_k) \geq 0
\end{align*}
$$
夹逼定理可以证明 $\{v_{\pi_k}\}_{k=0}^\infin = v^*$，同时也反映了策略迭代比值迭代具有更快的收敛速度（这也是因为它迭代内部还嵌套了一次迭代来计算 $v$），回答了问题 $3$

流程：
![image-20250822000444601](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250822000444601.png)

通过观察迭代的例子可以发现，更接近目标状态的状态会更早的做出最优的选择，原因是每个状态计算 action value 时依赖于其他状态的策略

## 三、截断策略迭代

值迭代和策略迭代都是截断策略迭代的一个特殊情况，对比这两个算法，令 $v_0 = v_{\pi_0}$ 方便比较：

1. 策略迭代初始化策略 $\pi_{0}$，值迭代还没开始
2. 策略迭代计算 $v_{\pi_0}$，值迭代初始化 $v_0$
3. 每轮迭代的修改策略阶段，对各自的 $v_{\pi_k},v_k$ 进行同样的操作寻求最“佳”策略 $\pi_{k+1}$
4. 每轮迭代的修改状态价值阶段，策略迭代计算当前策略对应的 state value $v_{\pi_{k+1}}$，值迭代根据旧的 $v_k$ 以及新的策略 $\pi_{k+1}$ 去推理新的 $v_{k+1}$，这里 $v_{\pi_{k+1}} \geq v_{k+1}$

去分析策略迭代的第一次迭代的 PI，这是一个迭代过程：
$$
\begin{align*}
v_{\pi_1}^{(0)} &= v_0 \\
\text{value iteration} \leftarrow v_1 \leftarrow \space v_{\pi_1}^{(1)} &= r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(0)} \\
v_{\pi_1}^{(2)} &= r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(1)} \\
&\vdots \\
\text{truncated policy iteration} \leftarrow \bar{v}_1 \leftarrow \space v_{\pi_1}^{(j)} &= r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(j-1)} \\
&\vdots \\
\text{policy iteration} \leftarrow v_{\pi_1} \leftarrow \space v_{\pi_1}^{(\infty)} &= r_{\pi_1} + \gamma P_{\pi_1} v_{\pi_1}^{(\infty)}
\end{align*}
$$
值迭代相当于直接取了第一次迭代的结果，这时可以想到其实可以中间取任意一个数 $j$ 进行截断处理，将此时的 $\bar{v}_1$ 作为新得到的 state value，如此获得 $v$ 去迭代的方法就是截断策略迭代，而值迭代就是 $j = 1$，策略迭代就是 $j = \infin$
$$
v_{\pi_k}^{(j+1)} - v_{\pi_k}^{(j)} = \gamma P_{\pi_k} (v_{\pi_k}^{(j)} - v_{\pi_k}^{(j - 1)}) = \cdots = \gamma^j P_{\pi_k}^j (v_{\pi_k}^{(1)} - v_{\pi_k}^{(0)}) \\
\text{由于 } \pi_{k} = \underset{\pi}{\text{argmax}}(r_{\pi} + \gamma P_{\pi}v_{\pi_{k-1}}) \\
v_{\pi_k}^{(1)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(0)} = r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_{k-1}} \geq r_{\pi_{k-1}} + \gamma P_{\pi_{k-1}} v_{\pi_{k-1}} = v_{\pi_k}^{(0)} \\
\text{故有 } v_{\pi_k}^{(j+1)} \geq v_{\pi_k}^{(j)}, \forall j \in N^*
$$
对于该算法的收敛性的证明比较复杂，之后再补充，但是由上述这个性质可以感受到它是介于策略迭代后值迭代之间的一种算法，通过调节 $j$ 可以控制收敛到最优的速度（迭代次数）（其实也没有节省，因为 $j$​ 大了每轮的计算量也大了）

流程：

![image-20250822024027618](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250822024027618.png)

一个图可以比较显示的表示三者之间的关系（从同一 $v_0$ 出发）：


<img src="C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250822024250452.png" alt="image-20250822024250452" style="zoom:70%;" />

# 5.蒙特卡洛方法

这是一个 model-free 的方法，如何在没有模型的情况下去估计模型？

蒙特卡罗思想就是通过采样来估计期望，而为什么要估计期望呢，因为 state value 和 action value 本身就是期望，它的关键是怎么将策略迭代算法中的模型替换为 model-free，这里采样具体指每采取一个行动，环境都会返回一个即时奖励

对于策略迭代算法 $q_{\pi_k}(s,a)$ 是一个关键，对于模型已知的算法之前就知道，而对于模型未知，从本质出发 $q_{\pi_k}(s,a) = \mathbb E[G_t|S_t=s,A_t=a]$，从$(s,a)$ 出发，根据 $\pi_{k}$ 产生一个轨迹，环境会返回 $g(s,a)$，这是 $G_t$ 的一个采样，当有很多条轨迹后可以去计算期望来估计 $q_{\pi_k}(s,a)$，在统计学习里这叫样本，而在强化学习中称为经验

## MC Basic 算法

给一个初始 $\pi_0$ 对于第 $k$ 次迭代：

1. policy evaluation：对于每对 $(s,a)$ 都要进行大量采样去估计 $q_{\pi_k}(s,a)$​
2. policy improvement：$\pi_{k+1}(s) = \underset{\pi}{\text{argmax}}\underset{a}{\sum}\pi(a|s)q_{\pi_k}(s,a)$ 依旧是确定性执行 $q$ 最大的动作

与之前的不同是在有模型的时候第一步是求解得到 $v_{\pi_k}$ 而现在是去估计 $q_{\pi_k}(s,a)$ 其实都是一样的，为了可以求解第二步来提升策略，从公式的角度它们是完全一致的

流程：

![image-20250822033945827](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250822033945827.png)

容易理解，但是效率很低（这个是赵老师自创的一种方法，方便循序渐进去理解）

在之前的讨论中轨迹的长度都是无限的，但这是无法采样的，现在考虑如何设置长度，可以肯定的是当步长越大时，对于 $q$ 的估计越准确，而当步长很小的时候，距离目标状态比较远的状态根本到达不了，得到的策略也不够好，所以步长一定要足够长但也不要无限，根据具体情况而定

下面考虑优化 MC Basic，对于一个 episode：
$$
s_1 \overset{a_2}{\rightarrow} s_2 \overset{a_2}{\rightarrow} s_1 \overset{a_2}{\rightarrow} s_2\overset{a_3}{\rightarrow} s_5 \overset{a_1}{\rightarrow} \cdots
$$
对于每个 state-action 对都称为对其的一次访问 vist，对于 MC Basic 方法，一次采样只得到了一次 $q_\pi(s,a)$ 的样本，但其实对于后续每个对都得到了一次采样结果，但被浪费了，可以将其也作为统计，有两种方法：

- first visit：对于每个 episode 每种 state-action 只统计一次
- every visit：对于每个 episode 每个 state-action 都统计

更新策略也可以优化，在 MC Basic 中需要将所有采样都得到后再去估计 action value，类似于 ML 里的随机梯度下降，可以每次只得到一个 episode 就去估计 action value，进行策略迭代，效率大大提升，这种方法和上一章的截断策略迭代都属于 Generalized policy iteratin(GPI) 的思想

## MC Exploring Starts

基于以上两个方面对 MC Basic 的一个优化，首先初始化 $\pi_0(a|s),q(s,a)$ 对于每个 state value 对，并设置变量来存储每个 $(s,a)$ 出现的频次以及总和。然后进入循环，每次循环采样一个 episode，有一个前提是采样的初始对 $(s_0,a_0)$ 保证所有对都有可能出现，从后往前遍历每个对：

1. 首先计算这个对的 discount return 采样结果 $g$，对于初值就是 $0$，每个对的结果等于 $\gamma g + r_{t+1}$
2. 修改对应的频次和总和
3. 修改对应的 $q(s_t,a_t)$ 进行 policy evaluation
4. 进行 policy improvement

流程：

![image-20250822161238401](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250822161238401.png)

注意上述的 exploring-starts 条件，很明显是因为如果有个 action value 是很优的但没有被访问到，那得到的策略显然不是最好，但是环境又是未知的，所以很那保证这一点，就需要从每个 $(s,a)$ 都开始一次 episode，下面的算法就是要优化这一点：
soft policy 属于随机策略，如果策略采取 soft policy 同时又要求长度尽量长，就可以去除 exploring-starts 条件了，这里采用的 soft policy 是 $\varepsilon$-greedy policy：
$$
\pi(a|s) =
\begin{cases}
1 - \dfrac{\varepsilon}{|\mathcal{A}(s)|}(|\mathcal{A}(s)| - 1), & \text{for the greedy action}, \\
\dfrac{\varepsilon}{|\mathcal{A}(s)|}, & \text{for the other } |\mathcal{A}(s)| - 1 \text{ actions}.
\end{cases} \\
\text{where } \varepsilon \in [0, 1] \text{ and } |\mathcal{A}(s)| \text{ is the number of actions for } s.
$$
相当于还是按照正常的 greedy policy 求解方法先找到最优的 action，将其它动作的概率平分 $\varepsilon$（总体还有少一份，保证最优动作概率最大），这样每种 $q(s,a)$ 都能采样到，如果这是一个真正好的动作，即使基于目前的策略，它的收益并不高，也会向这个方向去偏移

 **$\varepsilon$-greedy policy 平衡了 exploitation 与 exploration，充分利用最大动作的同时也保留了一些探索**， $\varepsilon = 0$ 就是 greedy policy，探索性消失了， $\varepsilon = 1$ 探索性很强，但是减少了充分利用性

## MC $\varepsilon$​-Greedy

相比于 MC Exploring Starts 就是改变了 policy improvement 的策略修改方式并省掉了 exploring-starts 条件，其他完全一致

流程：

![image-20250822172043619](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250822172043619.png)

这个算法的优点是具有探索性，但同时也牺牲了一部分最优性，通过 $\varepsilon$ 去调节，对于最终得到的 $\varepsilon$-greedy policy，可以选取最大动作再转为 greedy policy，用于实际任务

对于上面两种算法都没有约束 every visit 还是 first visit，一般默认是 first visit，更稳健，every visit 数据量大，不过重复且方差大

# 6.随机近似与随机梯度下降

这节课更多的是一些数学工具的学习，为之后的内容打个基础，并不局限于强化学习领域，与之前的章节不连续，但与后续连续

继续研究 mean estimation 问题，对于 $\bar x$ 的求解，最简单的就是将采样得到的数据求平均，但是采样过程一般都是漫长的，要等待全部采用完成后再求和求平均。可以用一种增量式或者迭代式的方法去克服：
$$
\text{令 } w_{k+1} = \frac{1}{k}\underset{i=1}{\overset{k}{\sum}}x_i,k=1,2,\dots \\
\text{有 } w_{k} = \frac{1}{k-1}\underset{i=1}{\overset{k-1}{\sum}}x_i,k=2,3,\dots \\
$$

$$
\begin{align*}
w_{k+1} = \frac{1}{k}\underset{i=1}{\overset{k}{\sum}}x_i &= \frac{1}{k}(\underset{i=1}{\overset{k-1}{\sum}}x_i + x_k) \\
&= \frac{1}{k}((k-1)w_k + x_k) = w_k - \frac{1}{k}(w_k - x_k)
\end{align*}
$$

所以在 $w_1 = x_1$ 时，可以使用迭代式的算法边采样边求解，此外可以证明对于 $\alpha_k > 0$，在满足一些条件后，按照 $w_{k+1} = w_k - \alpha_k(w_k - x_k)$ 去迭代依然可以逼近 $\mathbb E[X]$，之后会介绍。这其实·就是一种随机近似算法，也是一种随机梯度下降算法（$mean\space estimation \in SGD \in RM \in SA$）

随机近似（SA）代表了一大类随机（对随机变量采样）、迭代并用于对方程求解或优化的算法，在 ML 中最常见的对方程求解或优化的算法就是梯度下降，但是 SA 不需要知道这个方程或目标函数的表达式，在没有方程或目标函数的表达式的情况下显然也不知道梯度，此时梯度下降就无效了

Robbins-Monro(RM) 算法是 SA 算法中非常具有开创性的一个工作，随机梯度下降方法（SGD）就是 RM 的一种特殊情况，刚刚介绍的求解期望的算法也是。

给出一个问题：
$$
g(w) = 0
$$
$w \in \mathbb R$ 是要求解的未知量，$g: \mathbb R \rightarrow \mathbb R$ 是一个未知的函数，但是可以知道代入一个具体的 $w_0$ 后 $g(w_0)$ 的值，将 $g(w)$ 想象为一个神经网络，$w$ 是输入，经过网络得到输出 $y = g(w)$ 可以更好的理解这一求解过程，我们的目标就是要找到解 $w^*$ 使得 $g(w^*) = 0$

正式介绍 RM 算法，这是一个迭代式算法，它可以解决上述问题：
$$
w_{k+1} = w_k - a_k\tilde g(w_k,\eta_k),k = 1,2,3,\dots
$$
其中 $w_k$ 是根的第 $k$ 次估计值，$\tilde g(w_k,\eta_k) = g(w_k) + \eta_k$ 是第 $k$ 次带噪声的观测值，$a_k$ 是一个正系数，这里有噪声 $\eta$， 是由于对于传感器/仿真本身会有误差、强化学习环境具有随机性，又或者是 MC 样本均值与真实期望，总之各种情形下都是很难获取到真实的 $g(w_k)$ 的，用一个 $\eta$ 来表示之间的差距，服从什么分布未知（我们希望的目标依然是找到 $w^*$ 使 $g(w^*) = 0$，即使 $\tilde g(w^*,\eta) \ne 0$，因为 $g(w) = 0$ 才是最好的方案或者方程的解，只是我们无法观测到这个真实值）

![image-20250823003200493](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250823003200493.png)

下面分析 RM 算法的收敛性以及为什么能找到最优解，在 RM 算法中如果满足以下三个条件：
$$
0 < c_1 \leq \nabla_wg(w) \leq c_2,\forall w \\
\sum^{\infin}_{k=1}a_k = \infin \space \and \space \sum^{\infin}_{k=1}a_k^2 < \infin \\
\mathbb E[\eta_k|\mathcal H_k] = 0 \space \and \space \mathbb E[\eta_k^2|\mathcal H_k] < 0
$$
其中 $\mathcal H_k = {w_k,w_{k-1},\dots}$，此时 $w_k$ converges with probability $1$(w.p.1) to the root $w^*$ satisfying $g(w^*) = 0$，为什么这么写是因为这里涉及到一个随机变量的采样，所以这个收敛也不是常规意义的收敛，w.p.1 也是一个常用的缩写，下面对其解读：
对于条件 $1$ 就是要求导数是正数且有界的，即函数递增且不发散，又由于保证了导数处处大于等于一个常数，所以有且仅有一个零点，注意这里必须是要求 $g$ 递增，递减不行（不过加个负号就好了）

对于优化问题 $\text{min }J(w)$ 的思路就是转变为求解 $g(w) = \nabla_w J(w) = 0$，$\nabla_w J(w) = 0$ 对于一个连续函数这是极值点的必要条件，此时要求 $\nabla_w g(w) > c_1$，就是对 $J$ 的二阶导，得到 $J$​ 的 Hesian 矩阵，当它是一个正的矩阵时意味着这是一个凸函数，上述的必要条件也转变为了充要条件

对于条件 $2$ 中右边 $\sum^{\infin}_{k=1}a_k^2 < \infin$ 就是保证 $a_k$ 会趋于 $0$ 而 $\sum^{\infin}_{k=1}a_k = \infin$ 是限制 $a_k$ 不要收敛的太快。由于 $w_{k+1} - w_k = - a_k\tilde g(w_k,\eta_k)$ 若 $a_k$ 趋于 $0$ 则 $w_k$ 是收敛的，而为什么不能收敛太快呢，可以将整个迭代过程两边相加得到 $w_1 - w_\infin = \sum_{k=1}^{\infin}a_k\tilde g(w_k,\eta_k)$，如果可以收敛则 $w^* = w_\infin$，那此时如果 $a_k$ 的和是有界的，$\tilde g(w_k,\eta_k)$ 也是有界的，则初始猜测值和真实值的差也是有界的，此时对于初值 $w_1$ 就不能随意设置了，而如果是发散的，随意初始化都是有可能收敛到 $w^*$ 上的，最常见的就是 $a_k = \frac{1}{k}$，但实际通常令 $a_k$ 为一个很小的常数，原因是对于趋于 $0$ 的设计，后面的迭代起到的作用就很小了

对于条件 $3$ 要求其均值为 $0$ 同时方差有界

当三个条件有一个不满足时 RM 就可能不工作了

回顾之前提到的策略估计算法，$w_{k+1} = w_k - \alpha_k(w_k - x_k)$，考虑 $g(w) = w - \mathbb E[X]$，如果 $g(w) = 0$，此时的 $w^* = \mathbb E[X]$，但 $\mathbb E[X]$ 是未知的，于是可以采样 $\tilde g(w,x) = w - x$，这个例子也可以很具体的理解噪声的来源，你是无法得到 $g(w)$​ 的准确结果的，此时的 $\eta$ 就是采样与真实值的差异：
$$
\begin{align*}
\tilde g(w,\eta) = w - x &= w - x + \mathbb E[X] - \mathbb E[X] \\
&= (w - \mathbb E[X]) + (\mathbb E[X] - x) = g(w) + \eta
\end{align*}
$$
利用 RM 算法去迭代：
$$
w_{k+1} = w_k - a_k\tilde g(w_k,\eta_k) = w_k - a_k(w_k - x_k)
$$
这便是之前提到的算法，只要满足 $a_k$ 具有以上性质即可

以上对于 RM 算法的可行性更多的是定性的解释，后续有空学习 Dvoretzky's Therom 的证明这是证明 RM 定理的工具，然后学习 RM 的论证方法，现在要知道的就是它真的是在数学上能严格证明的能收敛的，虽然看上去不是很靠谱,下面介绍 SGD，考虑以下问题：
$$
\underset{w}{\text{min}} J(w) = \mathbb E[f(w,X)]
$$
其中 $w$ 是要被优化的参数，$X$​ 是随机变量，它的分布确定但未知

## 梯度下降（GD）

$$
w_{k+1} = w_k - \alpha_k \nabla_w \mathbb{E}[f(w_k, X)] = w_k - \alpha_k \mathbb{E}[\nabla_w f(w_k, X)]
$$

但这是基于模型的，如果没有模型就需要别的方法：

## 批量梯度下降（BGD）

$$
\mathbb{E}[\nabla_w f(w_k, X)] \approx \frac{1}{n} \sum_{i=1}^n \nabla_w f(w_k, x_i) \\
w_{k+1} = w_k - \alpha_k \frac{1}{n} \sum_{i=1}^n \nabla_w f(w_k, x_i)
$$

没有模型就要基于数据，利用微小变化去采样梯度，但是要采样很多次（在模型训练中其实是知道表达式 $f$ 的，就是损失函数，但是对于 $X$ 也就是输入的分布是不知道的，这依然是不知道模型的，需要基于数据，不过比连表达式都不知道的情况要好的是可以在固定 $w,x$ 后采样到准确的梯度，而不用数值上获取梯度），还需优化：

## 随机梯度下降（SGD）

$$
w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k)
$$

和 MC 的优化过程很像，用一个数据来估计 $\mathbb E$​，下面证明 SGD 算法属于 RM 算法即可证明收敛性（还有一种直接方法是证明 SGD 的收敛性，相当复杂，之后闲的可以看看）：

对于 DG，BGD，SGD，它们要解决的问题是一样的，令 $g(w) = \nabla_w J(w) = \mathbb E[\nabla_w f(w,X)]$，SGD 的目标就是找到 $w^*$ 使 $g(w^*) = 0$，利用 RM 的思想：
$$
\begin{align*}
\tilde g(w,\eta) &= \nabla_w f(w,x) \\
&= \underbrace{\mathbb E[\nabla_w f(w,X)]}_{g(w)} + \underbrace{\nabla_w f(w,x) - \mathbb E[\nabla_w f(w,X)]}_\eta
\end{align*}
$$

$$
w_{k+1} = w_k - a_k\tilde g(w_k,\eta_k) = w_k - a_k\nabla_w f(w_k,x_k)
$$

与 SGD 完全相同，是一种求解特殊问题的 RM 算法，只要保证那三个条件都满足就会收敛：
$$
0 < c_1 \leq \nabla^2_wf(w,X) \leq c_2 \\
\sum^{\infin}_{k=1}a_k = \infin \space \and \space \sum^{\infin}_{k=1}a_k^2 < \infin \\
\{x_k\}^\infin_{k=1}\text{ is iid}
$$
至于为什么这里第三条只要求 iid 需要完全理解 RM 收敛性的证明才能懂

上述已经证明了 SDG 最终可以 w.p.1 收敛到 $w^*$，下面分析有没有可能 SGD 在更新的过程中有很大一段时间是在向最优解完全相反的方向更新的，假设估计的期望与真实的期望的相对误差：
$$
\delta_k = \frac{|\nabla_w f(w_k,x_k) - \mathbb E[\nabla_w f(w_k,X)]|}{|\mathbb E[\nabla_w f(w_k,X)]|}
$$
由于最优解 $w^*$ 满足 $\mathbb E[\nabla_w f(w^*,X) = 0$ 有：
$$
\delta_k = \frac{|\nabla_w f(w_k,x_k) - \mathbb E[\nabla_w f(w_k,X)]|}{|\mathbb E[\nabla_w f(w_k,X)] - \mathbb E[\nabla_w f(w^*,X)|}
$$
根据拉格朗日中值定理，令 $\tilde w_k \in [w_k,w^*]$ 有：
$$
\delta_k = \frac{|\nabla_w f(w_k,x_k) - \mathbb E[\nabla_w f(w_k,X)]|}{|\mathbb E[\nabla^2_w f(\tilde w_k,X)(w_k - w^*)]|}
$$
这里假设 $\nabla^2_w f \geq c >0$，也是 RM 的条件：
$$
\begin{align*}
|\mathbb E[\nabla^2_w f(\tilde w_k,X)(w_k - w^*)]| &= |\mathbb E[\nabla^2_w f(\tilde w_k,X)](w_k - w^*)| \\
&= |\mathbb E[\nabla^2_w f(\tilde w_k,X)]||(w_k - w^*)| \\
&\geq c|w_k - w^*|
\end{align*}
$$

$$
\delta_k \leq \frac{|\overbrace{\nabla_w f(w_k,x_k)}^{随机梯度} - \overbrace{\mathbb E[\nabla_w f(w_k,X)]}^{真实梯度}|}{\underbrace{c|w_k - w^*|}_{\text{到最优解的距离}}}
$$

当 $|w_k - w^*|$ 很大时，相对误差比较小，此时的估计比较准确，更新方向基本正确，而当 $|w_k - w^*|$ 为 $0$ 很接近的时候，确实会有较大的随机性

下面考虑另一个问题：
$$
\underset{w}{\text{min }} J(w) = \frac{1}{n}\sum^{n}_{i=1}f(w,x_i)
$$
其在 $w$ 是要优化的参数，$\{x_i\}^n_{i=1}$ 是一组确定的序列，与之前唯一的区别就是不再有随机变量了而全部是固定的 $x_i$，此时的 GD 方法就是：
$$
w_{k+1} = w_k - \alpha_k \nabla_w J(w_k) = w_k - \alpha_k \frac{1}{n}\sum^n_{i=1} \nabla_w f(w_k, x_i)
$$
但实际上 $n$ 可能非常大，可以将上述的 $\alpha_k \frac{1}{n}\sum^n_{i=1} \nabla_w f(w_k, x_i)$ 改为 $\alpha_k \nabla_w f(w_k, x_k)$ 相当于做了一次采样，考虑几个问题：

1. 这样算 SGD 吗
2. 按什么方式去取 $x_k$

**这里先做一下区分，对于网络结构和损失函数的设计，是从各种角度最终得到 损失函数 $J(w)$（$f$ 是对单个样本的损失），这里讨论的是已经有了 $J(w)$ 怎么训练能更高效的收敛到最优值上，这两部分是解耦的**

我们可以引入一个随机变量 $X,p(X = x_i) = 1/n$，则原始等价于：
$$
\frac{1}{n}\sum^{n}_{i=1}f(w,x_i) = \mathbb E[f(w,X)]
$$
故此时就是原本的问题（完全等价），那自然通过随机采样得到的就是 SGD 算法，抽取方式是随机不放回抽取

下面对 BGD，SGD，MBGD 进行对比：

假设我们想要最小化 $J(w)=\mathbb E[f(w,X)]$，已知一组随机样本 $\{x_i\}^n_{i=1}$ 来自分布 $X$，用于解决此问题的 BGD、SGD 和 MBGD 算法分别是：
$$
w_{k+1} = w_k - \alpha_k \frac{1}{n} \sum_{i=1}^n \nabla_w f(w_k, x_i) \\
w_{k+1} = w_k - \alpha_k \frac{1}{m} \sum_{j \in \mathcal{I}_k} \nabla_w f(w_k, x_j) \\
w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k)
$$
MBGD 就是在 BGD 与 SGD 的一个折中，每次从集合中随机抽取一定量的数据（每个 mini-batch 的抽取一般是不放回的），灵活且高效

# 7.时序差分方法（TD）

继 MC 后的第二种 model-free 方法，还是考虑求解 $w = \mathbb E[X]$，拥有的数据是 iid 的 $\{x\}$，令 $g(w) = w - \mathbb E[X]$ 利用 RM 算法直接求解，下面考虑更复杂的一个情况 $w = \mathbb E[v(X)]$，同样拥有 iid 的 $\{x\}$，解决方案也是一样的，最后引出要求解的式子：
$$
w = \mathbb E[R+\gamma v(X)]
$$
其中 $R,X$ 均是随机变量，当拥有 $R,X$ 的 iid 采样后依然可以求解：
$$
\begin{align}
g(w) &= w - \mathbb{E}[R + \gamma v(X)] \\
\tilde{g}(w, \eta) &= w - [r + \gamma v(x)] \\
&= (w - \mathbb{E}[R + \gamma v(X)]) + (\mathbb{E}[R + \gamma v(X)] - [r + \gamma v(x)]) \\
&\doteq g(w) + \eta
\end{align}
$$
迭代求解的过程就是：
$$
w_{k+1} = w_k -\alpha_k\tilde g(w_k,\eta_k) = w_k - \alpha_k[w_k - (r_k + \gamma v(x_k))]
$$
TD 算法是一大类 RL 算法

## TD learning of state values

在给定的 $\pi$ 下求解 state value 的算法，相当于用于 policy evaluation，与 policy improvement 结合就可以求解最优策略了，在本块提到的 TD 没有特别说明都是专指这种算法

需要的数据是 $(s_0,r_1,s_1,\dots,s_t,r_{t+1},s_{t+1},\dots)\space or \space\{(s_t,r_{t+1},s_{t+1})\}$ 根据给定的 $\pi$ 产生，TD 算法：
$$
\begin{cases}
v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - [r_{t+1} + \gamma v_t(s_{t+1})]]&(1)\\
v_{t+1}(s) = v_t(s),\forall s \ne s_t &(2)
\end{cases}
$$
其中 $s$ 是 state space 中任意一个状态，$v_t$ 是 $t$ 时刻对 $v_\pi$  的估计值，$t=0,1,2,\dots$，$s_t$ 是 $t$ 时刻访问到的状态，上述的式子就是在 $t+1$ 时刻按某种方法去修改 $s_t$ 的 state value，而其他状态的 $v$ 不变，第二个式子一般都省略：
$$
\underbrace{v_{t+1}(s_t)}_{\text{new estimate}} = \underbrace{v_t(s_t)}_{\text{current estimate}} - \alpha_t(s_t)[\overbrace{v_t(s_t) - [\underbrace{r_{t+1} + \gamma v_t(s_{t+1})}_{\text{TD target }\bar v_t}]}^{\text{TD error }\delta_t}]
$$
总体来看就是新的估计由旧的估计加上一个修正项得到，$\bar v_t$ 称为 TD target，并希望 $v(s_t)$ 向着 $\bar v_t$ 改进，解释第二点：
$$
\begin{align}
&v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - \bar v_t] \\
&v_{t+1}(s_t) - \bar v_t = v_t(s_t) - \bar v_t - \alpha_t(s_t)[v_t(s_t) - \bar v_t] \\
&v_{t+1}(s_t) - \bar v_t = [1 - \alpha_t(s_t)][v_t(s_t) - \bar v_t] \\
&|v_{t+1}(s_t) - \bar v_t| = |1 - \alpha_t(s_t)||v_t(s_t) - \bar v_t| \\
\end{align}
$$
由于 $\alpha_t(s_t)$ 是一个很小的正数，有 $0 < \alpha_t(s_t) < 1$，所以：
$$
|v_{t+1}(s_t) - \bar v_t| \leq |v_t(s_t) - \bar v_t|
$$
说明 $v_(s_t)$ 确实是向着 $\bar v_t$ 改进，对于 TD error：
$$
\delta_t = v_t(s_t) - [r_{t+1} + \gamma v_t(s_{t+1})]
$$
表示的是两个时间步上的差分，这也是为什么称为时序差分算法，反应了 $v_t$ 与 $v_\pi$ 之间的误差，下面解释这一点：
$$
\text{定义 }\delta_{\pi,t} = v_\pi(s_t) - [r_{t+1} + \gamma v_\pi (s_{t+1})] \\
\mathbb E[\delta_{\pi,t}|S_t = s_t] = v_\pi(s_t) - \mathbb E[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s_t] = 0
$$
也就是当 $v_t = v_\pi$ 时，$\mathbb E(\delta_t) = 0$，以上都是定性的去解释这个公式，下面从数学角度去说明：

TD 其实解决的是一个给定 $\pi$ 的贝尔曼公式，也就是一种在没有模型的情况下求解贝尔曼公式的算法，考虑 $v_\pi$ 的定义 ：
$$
v_\pi(s) = \mathbb E[R_{t+1} +\gamma G_{t+1}|S_t = s],s \in \mathcal S \\
\mathbb E[G_{t+1}|S_t = s] = \sum_a\pi(a|s)\sum_{s'}p(s'|s,a)v_\pi(s') = \mathbb E[v_\pi(S_{t+1})|S_t = s] \\
v_\pi(s) = \mathbb E[R_{t+1} +\gamma v_\pi(S_{t+1})|S_t = s],s \in \mathcal S 
$$
转化为这种形式的贝尔曼方程又称为贝尔曼期望方程，使用 RM 算法去进行求解：
$$
g(v(s)) = v(s) - \mathbb E[R_{t+1} +\gamma v_\pi(S_{t+1})|S_t = s]
$$
和 $w$ 一样，$v(s)$ 就是一个向量，我们只能采样得到 $r_{t+1},s_{t+1}\space of \space R_{t+1},S_{t+1}$：
$$
\begin{align*}
\tilde{g}(v(s)) &= v(s) - [r_{t+1} + \gamma v(s_{t+1})] \\
&= \underbrace{(v(s) - \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) |S_t = s])}_{g(v(s))} + \underbrace{(\mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) |S_t = s] - [r_{t+1} + \gamma v_{\pi}(s_{t+1})])}_{\eta}
\end{align*}
$$
后续为方便用 $s',r,S',R$ 表示 $s_{t+1},r_{t+1},S_{t+1},R_{t+1}$，因此迭代过程为：
$$
\begin{align*}
v_{k+1}(s) &= v_k(s) - \alpha_k\tilde{g}(v(s)) \\
&= v_k(s) - \alpha_k(v_k(s) - [r_k + \gamma v_\pi(s'_k)]),k = 1,2,3,\dots
\end{align*}
$$
这与一般的 RM 会有两点区别：

1. 不能固定 $s$ 去采样 $s',r$，只能通过轨迹构建 $\{(s_t,r_{t+1},s_{t+1})\}$，但当 $s$ 为其他值时，如 $s \ne s_0$ 时，对 $v(s_0)$ 是不发生变化的，所以相当于同时去迭代 $|\mathcal S|$ 个 RM 算法，碰到哪个 $s$ 就去迭代对应的 $v(s)$，而对于每个 RM 算法来看就是随机采样 $s',r$​ 并连续的迭代
2. 式子中出现了 $v_\pi(s_k')$ 是不知道的，只能用 $v_k(s_k')$ 去估计了，但这就不再是传统的 RM 算法了，收敛性有待考证，但结论就是可以收敛，意会一下就是每个状态的 state value 都在向真实的趋近，只要迭代次数够多，总体的趋势会慢慢接近共同进步的

给出 TD 定理：在 $t \rightarrow \infin$ 时对于 $\forall s \in \mathcal S$，$v_t(s)$ w.p.1 收敛到 $v_\pi(s)$ ，如果满足：
$$
\sum_t\alpha_t(s) = \infin \space \and \space \sum_t\alpha_t^2(s) < \mathcal S,\forall s \in \mathcal S
$$
上述中 $\alpha_t(s)$ 在 $t$ 次迭代访问的是 $s$ 时为具体的值，没访问到的时候是 $0$，证明之后再补充

通常对于求和为无穷隐含的信息就是每个状态都要访问无数次，但实际中一般都是令其为一个很小的正常数，显然此时不满足这个条件了，这是根据经验设置的，目的也是希望后续的迭代能产生作用，此时就一定可以收敛到 $0$ 了，但实际中也不可能迭代无数次，所以通过一个很小的阈值来判断终止依然有效

| TD                                                           | MC                                                           |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| online，对于每次跳转获得即时奖励就可以更新                   | offline，需要等待完整的轨迹采样完成                          |
| 可以用于 continuing task，不用再限制轨迹长度，当然也可以用于 episode task | 只能用于 episode task                                        |
| boostrapping，在之前的估计值上更新                           | non-boostrapping                                             |
| 方差较小，随机变量的取值数量较少                             | 方差大，$q_\pi(a,s)$ 的估计涉及到后续一串的即时奖励，随机变量取值很广 |
| 有偏估计                                                     | 无偏估计                                                     |

## TD learning of action values​：Sarsa

 Sarsa 及其变型都是根据给定的策略去估计 action value，假设具有数据 $\{(s_t,a_t,r_{t+1},s_{t+1},a_{t+1}\}$，Sarsa 名字的来源也是这五元组的首字母，表示的一次迭代用到的数据，具体为：
$$
\begin{cases}
q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma q_t(s_{t+1},a_{t+1})]] \\
q_{t+1}(s,a) = q_t(s,a),\forall(s,a) \ne (s_t,a_t)
\end{cases}
$$
$t =  1,2,3,\dots$，$q_t(s_t,a_t)$ 表示 $t$ 时刻对 $q_\pi(s_t,a_t)$ 的估计，$\alpha_t(s_t,a_t)$ 是 $t$ 时刻 $(s_t,a_t)$ 对应的学习率，与 TD 比较其实就是将 $v$ 换为了 $q$，也是根据贝尔曼公式转化形式再利用 RM 算法得到的：
$$
\begin{align*}
q_\pi(s,a) &= \sum_r rp(r|s,a) + \gamma\sum_{s'} p(s'|s,a) \sum_{a'} \pi(a'|s') q_\pi(s',a') \\
&= \sum_r rp(r|s,a) + \gamma\sum_{s'} \sum_{a'} p(s'|s,a) \pi(a'|s') q_\pi(s',a') \\
&= \sum_r rp(r|s,a) + \gamma\sum_{s'} \sum_{a'} p(s',a'|s,a) q_\pi(s',a') \\
&= \mathbb E[R + \gamma q_\pi(S',A')|s,a], \text{ for all }(s,a)
\end{align*}
$$
关键的一步在于 $\pi(a'|s') = p(a'|s',s,a)$，得到上述式子后 利用 RM：
$$
g(q_\pi(s,a)) = q(s,a) - \mathbb E[R + \gamma q_\pi(S',A')|s,a] \\
\tilde g(q(s,a)) = q(s,a) - [r + \gamma q_\pi(s',a')] \\
q_{k+1}(s,a) = q_k(s,a) - \alpha_k(s,a)[q_k(s,a) - [r_k + \gamma q_\pi(s'_k,a'_k)]]
$$
依然是与 TD 一样的两个问题，解释也完全相同，所以最终迭代的表达式为：
$$
q_{k+1}(s,a) = q_k(s,a) - \alpha_k(s,a)[q_k(s,a) - [r_{k} + \gamma q_k(s'_k,a'_k)]]
$$
对应的收敛条件以及真实的设置也完全一致不再赘述

下面尝试利用 Sarsa 和 PI 结合寻找最优策略，这个合并后的算法依然叫 Sarsa，甚至 Sarsa 更多的情况下指的是合并后的算法：

![image-20250824023456446](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250824023456446.png)

## TD learning of action values：Expected Sarsa

直接给出迭代式：
$$
\begin{cases}
q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma \mathbb E[q_t(s_{t+1},A)]]] \\
q_{t+1}(s,a) = q_t(s,a),\forall(s,a) \ne (s_t,a_t)
\end{cases}
$$
区别就是将 $q_t(s_t,a_t)$ 变为了 $\mathbb E[q_t(s_{t+1},A)]$：
$$
\mathbb E[q_t(s_{t+1},A)] = \sum_a\pi_t(a|s_{t+1})q_t(s_{t+1},a) = v_t(s_{t+1})
$$
这里 $\pi_t$ 对于确定策略求 action value 时就是定的，与 PI 结合时就是每次迭代得到的新策略，这里 $v$ 只是一个标记不是真实的 state value。因为 RM 算法原本这个位置应该是 $q_\pi(s_{t+1},a_{t+1})$ 的，但是无法得到所以被替代了，而这里又进行了修改，计算量增加了，但是可以省去一个随机变量 $a' \text{ or } a_{t+1}$，随机性也可以降低

从贝尔曼公式的另一种变型也可以得到：
$$
\begin{align*}
q_\pi(s,a) &= \sum_r rp(r|s,a) + \gamma\sum_{s'} p(s'|s,a) \sum_{a'} \pi(a'|s') q_\pi(s',a') \\
&= \sum_r rp(r|s,a) + \gamma \sum_{s'} p(s'|s,a)\mathbb E[q_\pi(S_{t+1},A_{t+1})|S_{t+1} = s'] \\
&= \mathbb E[R_{t+1} + \gamma \mathbb E[q_\pi(S_{t+1},A_{t+1})|S_{t+1}]|S_t = s,A_t = a] \\
\end{align*}
$$
有因为：
$$
\mathbb E[q_\pi(S_{t+1},A_{t+1})|S_{t+1} = s'] = \sum_{a'}q_\pi(S_{t+1},a')\pi(a'|s') = v_\pi(S_{t+1})
$$
所以：
$$
\begin{align*}
q_\pi(s,a) &= \mathbb E[R_{t+1} + \gamma \mathbb E[q_\pi(S_{t+1},A_{t+1})|S_{t+1}]|S_t = s,A_t = a] \\
&= \mathbb E[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s,A_t = a]
\end{align*}
$$
按照 RM 算法最终得到迭代式

## TD learning of action values：$n$-step Sarsa

对于 action value 的定义：
$$
q_\pi(s,a) = \mathbb E[G_t|S_t=s,A_t=a]
$$
可以将 $G$ 一步步拆开：
$$
\begin{align*}
\text{Sarsa} \quad \quad G_t^{(1)} &= R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}), \\
G_t^{(2)} &= R_{t+1} + \gamma R_{t+2} + \gamma^2 q_{\pi}(S_{t+2}, A_{t+2}), \\
&\quad\quad\vdots \\
\text{n-step Sarsa} \quad G_t^{(n)} &= R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n} q_{\pi}(S_{t+n}, A_{t+n}), \\
&\quad\quad\vdots \\
\text{MC} \quad G_t^{(\infty)} &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} \ldots
\end{align*}
$$
根据不同的展开程度进行 RM 算法就得不同算法（不过 MC 是直接估计 $q$ 的，不使用 RM），对于 $n$-step Sarsa：
$$
q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^n q_t(s_{t+n},a_{t+n})]]
$$
对于 $n$-step Sarsa 需要的数据为 $(s_t,a_t,r_{t+1},s_{t+1},a_{t+1},\dots,r_{t+n},s_{t+n},a_{t+n})$，在 $t+n$ 时刻才能对 $t$​ 时刻更新，是 online 与 offline 的一个折中，性质也位于两者之间，依然可以通过与 PI 结合对最优策略搜索

## TD learning of optimal action values：Q-learning

这是直接对最优的 action value 进行估计，不再需要 PE，PI 交替，直接给出算法：
$$
\begin{cases}
q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t) - [r_{t+1} + \gamma \underset{a \in \mathcal A}{\text{ max }}q_t(s_{t+1},a)]] \\
q_{t+1}(s,a) = q_t(s,a),\forall(s,a) \ne (s_t,a_t)
\end{cases}
$$
它不再是对贝尔曼公式变形而是对贝尔曼最优公式变形：
$$
\begin{align*}
v(s) &= \underset{\pi}{\text{max}} \sum_{a \in \mathcal A(s)}\pi(a|s)[\sum_rp(r|s,a)r + \gamma \sum_{s'}p(s'|s,a)v(s')] \\
&= \underset{a \in \mathcal A(s)}{\text{max}}[\sum_rp(r|s,a)r + \gamma \sum_{s'}p(s'|s,a)v(s')] \\
v(s) &= \underset{a \in \mathcal A(s)}{\text{max}}q(s,a) \\
\underset{a \in \mathcal A(s)}{\text{max}}q(s,a) &= \underset{a \in \mathcal A(s)}{\text{max}}[\sum_rp(r|s,a)r + \gamma \sum_{s'}p(s'|s,a)\underset{a \in \mathcal A(s)}{\text{max}}q(s,a)] \\
\end{align*}
$$
这是贝尔曼最优公式变换得到的，观察去掉 max 之后的含义，依然是成立的（$q$ 为最优策略对应的值），利用 RM 算法得到最优解

下面讨论 Q-learning 的一些性质，先介绍一下定义

behaviour policy：用于与环境交互得到经验的策略

target policy：被持续更新向最优策略靠近的策略

每个 model-free 算法都有对应的 behaviour policy 与 target policy

on-policy：behaviour policy 与 target policy 相同的算法，自己的作业（策略），批改之后（经验），自己改（策略）

off-policy：behaviour policy 与 target policy 不同的算法，用别人的作业（策略）被批改后（经验）作为参考，改自己的（策略）

off-policy 的好处就是可以从别人的经验中学习，如果 behaviour policy 很有探索性，就能探索到更多的可能性

MC、Sarsa 就是 on-policy 算法，而 Q-learning 是 off-policy 算法，前两者很好理解，从流程也能看出，解释第三点：Q-learning 和贝尔曼最优公式一样，都不显示的出现策略，需要的数据包括 $(s_t,a_t,r_{t+1},s_{t+1})$，对于 $(s_t,a_t)$ 给定后， $(r_{t+1},s_{t+1})$ 只与环境有关，Q-learing 对应的 behaviour policy 是任意策略都可以，使用其在 $s$ 上做出行为，然后根据生成的数据去修改 $q$，而 target policy 并没有显示存在，而是每一时刻 $q_t$ 对应的策略，当然在收敛之前 $q$ 并不一定是真实存在的 action value，但是它可以有对应策略（不过这个策略反过来算 $q$ 并不与其相等），显示定义就是：
$$
\pi_t(a|s) = 
\begin{cases}
1[\text{if }a = \underset{a'}{\text{argmax }}q_t(s,a')] \\
0[\text{else}]
\end{cases}
$$
当然如果同时有两个最大动作只能令其中一个为 $1$，当然也可以强行令 behaviour policy 为上述得出的 target policy，此时就是 on-policy:

![image-20250824153934047](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250824153934047.png)

和 Sarsa 算法基本一致，只是将 $q_t(s_{t+1},a_{t+1})$ 改为 $\underset{a}{\text{max }}q_t(s_{t+1},a)$，少了 $a_{t+1}$ 这个随机变量的采样，下面是 off-policy：

![image-20250824155805635](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250824155805635.png)

区别就是采样不再是利用被迭代的策略，而是单独指代一个策略，此外对策略的提升也不再是 $\varepsilon$-greedy，而是确定性的 greedy，可以兼顾探索性和充分利用

对以上的 TD 算法以及 MC 总结为一个统一的形式
$$
q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t) [q_t(s_t, a_t) - \bar q_t]
$$

其中 $\bar q_t$ 是 TD target，不同的 TD 算法有不同的 $\bar q_t$：
$$
\hline
\textbf{Algorithm} & \textbf{Expression of \(\bar q_t\)} \\ \hline
Sarsa & \bar q_t = r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1}) \\ \hline
n-step Sarsa & \bar q_t = r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^n q_t(s_{t+n}, a_{t+n}) \\ \hline
Expected Sarsa & \bar q_t = r_{t+1} + \gamma \sum_a \pi_t(a|s_{t+1}) q_t(s_{t+1}, a) \\ \hline
Q-learning & \bar q_t = r_{t+1} + \gamma \max_a q_t(s_{t+1}, a) \\ \hline
Monte Carlo & \bar q_t = r_{t+1} + \gamma r_{t+2} + \cdots \\ \hline
$$

希望求解的表达式也可以统一：
$$
\hline
\textbf{Algorithm} & \textbf{Equation aimed to solve} \\ \hline
Sarsa & BE: q_{\pi}(s,a) = \mathbb{E}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1})|S_t = s, A_t = a] \\ \hline
n-step Sarsa & BE: q_{\pi}(s,a) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^n q_{\pi}(s_{t+n}, a_{t+n})|S_t = s, A_t = a] \\ \hline
Expected Sarsa & BE: q_{\pi}(s,a) = \mathbb{E}[R_{t+1} + \gamma\mathbb{E}_{A_{t+1}}[q_{\pi}(S_{t+1}, A_{t+1})]|S_t = s, A_t = a] \\ \hline
Q-learning & BOE: q(s,a) = \mathbb{E}[R_{t+1} + \gamma \max_a q(S_{t+1}, a)|S_t = s, A_t = a] \\ \hline
Monte Carlo & BE: q_{\pi}(s,a) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \cdots|S_t = s, A_t = a] \\ \hline
$$
总体的思想就是把希望得到的值用贝尔曼公式或贝尔曼最优公式写出表达式，作差构建 $g$ 函数，然后对于其中的随机变量用采样代替得到 $\tilde g$，有时采样不能随机采样，得到什么就去对应的 RM 进行迭代，对于每个式子依然是连续的，其中可能存在一些无法得知的值，利用当时可以去估计的值去代替（这一步与 RM 有些出入，但是经过证明依然可以收敛），这样就可以去估计希望得到的值，这一步是 PE，搭配 PI 就可以去求解最优策略

强化学习的 TD 算法与神经网络结合的时候通常使用 Q-learning，具体是由于它自身一些特殊的性质决定的，下一章来讲解

# 8.值函数近似

之前虽然没有明说，但其实 state value 和 action value 都可以看作用一个表格来统计的，编程中就是用数组去存储，好处是直观简单，但是对于很大的 action space 或 state space 或者连续空间来说是无法处理的，在存储和泛化能力（没遇到的就不知道怎么转移了）上都会有很大问题

先考虑 $v(s)$ 当 $|\mathcal S|$ 很大时，可以拟合一个曲线来对其表示，先考虑直线：
$$
\hat v(s,w) = as + b = \bigl[s,1\bigr]
\biggl[
\begin{matrix}
a \\
b
\end{matrix}
\biggr] = 
\underbrace{\phi^T(s)}_{\text{特征向量}}
\underbrace{w}_{\text{参数向量}}
$$
$w$ 为参数向量，$\phi(s)$ 是 $s$ 的特征向量，这样的好处是可以极大的节省存储 $v(s)$ 的空间，只要存两个参数，代价就是牺牲了很多精度，可以使用更高阶的，来增加 $\phi(s),w$ 的维度，此时对 $s$ 便是一个非线性的拟合，但对与 $w$ 依然是线性的。这种方式同时解决了存储问题和泛化能力

在确定模型结构后 $\hat v$ 表达式就确定，剩下的就是优化 $w$ 使 $\hat v(s,w)$ 接近 $v(s)$，

先去定义目标函数：
$$
J(w) = \mathbb E[(v_\pi(S) - \hat v(S,w))^2]
$$
$S$ 是一个随机变量，考虑其概率分布，最简单的就是平均分布：
$$
J(w) = \mathbb E[(v_\pi(S) - \hat v(S,w))^2] = \frac{1}{|\mathcal S|} \sum_{s \in \mathcal S} (v_\pi(S) - \hat v(S,w))^2
$$
但实际上每个状态的权重多数不一样，对于目标状态及其附近的权重肯定很高，而其他较偏的状态很少访问到，权重较小，可以采用 stationary distribution，让智能体在环境中不断交互，转移，到达的一个稳定的状态，可以求解每个状态被访问的概率，这就是 stationary distribution，依赖于 $\pi$，用 $d_\pi(s)$ 表示，权重越大，在目标函数的占比越大，越被重视，$n_\pi(s)$ 表示在策略 $\pi$ 下经过非常长的访问，在 $s$ 上访问的次数，则：
$$
d_\pi(s) \approx \frac{n_\pi(s)}{\sum_{s'\in \mathcal S}n_\pi(s')}
$$
具有如下性质（$P_\pi$ 是状态转移矩阵）：
$$
d^T_\pi = d^T_\pi P_\pi
$$
下面考虑怎么去优化目标函数，最容易想到的就是梯度下降：
$$
w_{k+1} = w_k - \alpha_k \nabla_w J(w_k)
$$
 对梯度进行一些变形：
$$
\begin{align*}
\nabla_w J(w) &= \nabla_w \mathbb{E}[(v_\pi(S) - \hat{v}(S,w))^2] \\
&= \mathbb{E}[\nabla_w (v_\pi(S) - \hat{v}(S,w))^2] \\
&= 2\mathbb{E}[(v_\pi(S) - \hat{v}(S,w))(-\nabla_w \hat{v}(S,w))] \\
&= -2\mathbb{E}[(v_\pi(S) - \hat{v}(S,w))\nabla_w \hat{v}(S,w)]
\end{align*}
$$
其中的求期望应避免，采用随机梯度（SGD）：
$$
w_{t+1} = w_t + \alpha_t(v_\pi(s_t) - \hat v(s_t,w_t))\nabla_w \hat v(s_t,w_t)
$$
其中 $2\alpha_k$ 合并为 $\alpha_k$，$s_t$ 是对 $S_t$ 的采样，就得到了最终的形式（依然是使用了 RM 的思想）

但是未知的，这里使用 $g_t$ 即用本次对 $s_t$​ 采样的整个轨迹的 return 作为估计值，属于 MC 的思想：
$$
w_{t+1} = w_t + \alpha_t(g_t - \hat v(s_t,w_t))\nabla_w \hat v(s_t,w_t)
$$
如果使用 TD 的思想：
$$
w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \hat v(s_{t+1},w_t) - \hat v(s_t,w_t)]\nabla_w \hat v(s_t,w_t)
$$
相当于用 $\hat v$ 的 TD target $r_{t+1} + \gamma \hat v(s_{t+1},w_t)$ 来表示 $v_\pi(s_t)$

**这里对 TD target 的含义再进行一次阐明：TD target 是用一步或几步的即时奖励 + 自举（bootstrapping）得到的，TD 的主要思想就是用 TD target 去作为估计值，而 MC 是通过对整条采样轨迹计算累计回报进行估计。**

流程：

![image-20250824182350442](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250824182350442.png)

这里就是根据采样的结果进行更新，没有采样到的状态也会与受到采样的状态一同更新

上述已经知道了如何更新，下面讨论如何设计 $\hat v$，最基本的就是线性方程（对 $w$ 来说）：
$$
\hat v(s,w) = \phi^T(s)w
$$
特征向量 $\phi(s)$ 就是一个关于 $s$ 的任意向量，有很多种选法，多项式是常见的一种，除了上述的线性方程，还可以通过一个神经网络（非线性）来获得，对于线性来说：
$$
\nabla_w \hat v(s,w) = \phi(s)
$$
代入到迭代过程：
$$
w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \hat v(s_{t+1},w_t) - \hat v(s_t,w_t)]\phi(s)
$$
称为 TD-Linear 算法，缺点是需要选取非常合适的 $\phi(v)$，这也是 linear function approximately 被神经网络取代的重要原因（传统机器学习被深度学习），好处是理论性质比较好分析，可解释性强，虽然不能近似所有函数但也具有很强的表征能力，基于表格的表示是 linear function approximately 的一个特殊情况，考虑一个特殊的特征向量：
$$
\phi(s) = e_s \in \mathbb R^{|\mathcal S|} \\
\hat v(s,w) = e_s^Tw = w(s)
$$
$e_s$ 表示第 $s$ 行为 $1$，其余为 $0$ 的向量（one-hot），$w(s)$ 表示 $w$ 的第 $s$ 项，就转化为一个表格了，代入到迭代式：
$$
\begin{align*}
w_{t+1} &= w_t + \alpha_t[r_{t+1} + \gamma \hat v(s_{t+1},w_t) - \hat v(s_t,w_t)]\phi(s) \\
&= w_t + \alpha_t[r_{t+1} + \gamma w_t(s) - w_t(s_t)]e_{s_t} \\
\end{align*}
$$
由于 $e_{s_t}$ 的形式可以看出只有 $w_{t+1}$ 被更新了：
$$
\begin{cases}
w_{t+1}(s_t) = w_t(s_t) + \alpha_t[r_{t+1} + \gamma w_t(s_{t+1}) - w_t(s_t)] \\
w_{t+1}(s) = w_t(s),\forall s \ne s_t
\end{cases}
$$
就是对于表格的 TD 算法（把 $q,v$ 换成了 $w$ 而已）

上述更多的是讲了一个故事，这里利用 MC，TD 的思想去替换 $v_\pi(s_t)$ 数学是不严谨的，在替换后，实际被优化的目标函数就变了，考虑几种目标函数：

//后续补充，对应8.4 11：00后的所有内容

上述的 TD 引入后实际优化的目标函数是：
$$
J_{PBE}(w) = ||\hat v(w) - MT_\pi(\hat v(w))||^2_D
$$
想理解深刻，去看一下矩阵论的最小二乘解（投影矩阵）

// 8.4 结束

## Sarsa with function approximately

Sarsa 与 function approximately 结合后的算法：
$$
w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \hat q(s_{t+1},a_{t+1},w_t) - \hat q(s_t,a_t,w_t)]\nabla_w \hat q(s_t,a_t,w_t)
$$
和上述的 TD 一样，只是 $v$ 变为了 $q$，现在做的依然是 PE，在给定的策略上采样估计其 action value，与 PI 结合可以估计最优策略：

![image-20250824202103393](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250824202103393.png)

与 Sarsa 的区别是在 PE 中不再是对 $q$ 进行迭代，而是对 $w$，在 PI 中再将 $w_{t+1}$ 代入到 $\hat q$ 中得到最大的 action value 对应的动作进行 $\varepsilon$-greedy

## Q-learning with function approximately

和 Sarsa 非常相似：
$$
w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \underset{a \in \mathcal A(s_{t+1})}{\text{ max }} \hat q(s_{t+1},a_{t+1},w_t) - \hat q(s_t,a_t,w_t)]\nabla_w \hat q(s_t,a_t,w_t)
$$
就是把 TD target 换为了 Q-learning 对应的，给出 on-policy 流程：

![image-20250824203218435](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250824203218435.png)

## Deep Q-learning(DQN)

第一个成功的将神经网络引入到强化学习的算法，神经网络在其中的角色就是提供了非线性方程，发明 DQN 而不直接用 Q-learning 的原因是现在神经网络的框架已经很完善，可以自动求梯度，而 Q-learning 需要根据不同的 $\phi(s,a)$ 去手动写梯度（至少是一个很重要的原因）

DQN 的目标函数是：
$$
J(w) = \mathbb E[(R + \gamma \underset{a \in \mathcal A(S')}{\text{max}}\hat q(S',a,w) - \hat q(S,A,w))^2]
$$
也就是希望 TD error 的期望尽量小，也就是之前提到的多个目标函数中的 Bellam optimality error，考虑优化这个目标函数，依然是梯度下降，对于其梯度的计算是 DQL 的一个关键，式中有两项包含，第二项求导很简单，对于第一项，令：
$$
y = R + \gamma \underset{a \in \mathcal A(S')}{\text{max}}\hat q(S',a,w)
$$
并假设 $y$ 中的 $w$ 是一个常数，为了实现这件事引入了两个 network(function)：

1. main network：$\hat q(s,a,w)$
2. target network：$\hat q(s,a,w_T)$

main network 的 $w$ 一直更新，target network 的 $w_T$ 隔一段时间用 $w$ 赋值一下：
$$
J = \mathbb E[(R + \gamma \underset{a \in \mathcal A(S')}{\text{max}}\hat q(S',a,w_T) - \hat q(S,A,w))^2] \\
\nabla_w J = \mathbb E[(R + \gamma \underset{a \in \mathcal A(S')}{\text{max}}\hat q(S',a,w_T) - \hat q(S,A,w))\nabla_w \hat q(S,A,w)]
$$
具体流程：

1. $w,w_T$ 初始化为相同的值
2. 每轮迭代，从 replay buffer（后面介绍）采样出一个 mini-batch $\{(s,a,r,s')\}$，对于每个采样，输入 $(s,a)$，得到 target $y_T = r + \gamma \underset{a \in \mathcal A(s')}{\text{max}}\hat q(s',a,w_T)$，获得一个 mini-batch $\{(s,a,y_T)\}$ ，可以计算出 $\nabla_w f(s,k,w)$，利用 MBGD 更新，训练一段时间后对 $w_T$ 进行一次更新

下面介绍另一个技巧 experience replay 经验回放，在采样获得一些经验的时候，获得是有顺序的，但是使用不一定要按照原顺序，将所有经验存储到一个集合中，称为 replay buffer $\mathcal B = \{(s,a,r,s')\}$，从 $\mathcal B$ 拿取样本的过程就称为经验回放，按照均匀分布去拿取

下面介绍为什么在 DQL 中要使用经验回放以及为什么使用均匀分布，这依赖于目标函数：
$$
J = \mathbb E[(R + \gamma \underset{a \in \mathcal A(S')}{\text{max}}\hat q(S',a,w) - \hat q(S,A,w))^2]
$$
这里那个 $S,A$ 当做索引，$(S,A) \sim d$ 为一个随机变量，$d$ 是什么后续再说，$R \sim p(A|S,A),S' \sim p(S'|S,A)$ 取决于模型本身不用考虑，对于 $(S,A)$ 在没有先验知识的前提下，最好的就是认为其符合均匀分布，所以采集也应该均匀分布，而经验池中自然就有实际的分布，通过几个问题来强化一下理解：

1. 为什么 Q-learning 不需要经验回放？

   没有 $(S,A)$ 均匀分布的需求

2. 为什么 DQL 就涉及分布（很关键）

   对于表格的 Q-learning 求解的是对于每一对$(s,a)$在最优策略下的 action value，虽然从最初的贝尔曼公式中依然是要求的随机采样，但只要迭代次数够多，不均匀依然是会收敛的，而对于 DQL 是要最小化一个标量方程，虽然无论什么分布实际的最优解是固定的，但$(s,a)$不同的分布会直接影响收敛的速度以及路径，而实际很难收敛到最优值，所以这个分布是会直接影响到收敛结果的，在没有先验知识的情况下均匀分布就是最好的

3. 基于表格的 Q-learning 可以使用经验回放吗？、

   显然可以，甚至更好，更充分利用数据，减少轨迹长度

给出 DQL 的 off-policy 版本的流程：

![image-20250824224502012](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250824224502012.png)

**学到这里本章基本就结束了，可以看出从最开始传统强化学习的严谨到后面的意会，在神经网络加入之后就不再有那么严谨的理论证明了，显然它是有理论的，但是过于复杂了，绝大多数都是人类未曾掌握的，所以学到的很多都是前人的经验，在已有的数学基础上，为了高效或是可训练进行一些化简，并与已经存在的一些经验进行组合，如果效果好那就是一个好的算法，然后再从框架试图从某些角度来解释有效的原因，作为后人使用以及设计新算法的经验**

本章与论文原文的 DQL 有一些出入，首先原文是 on-policy 的，这都无所谓很容易修改，而对于网络的输入输出也有一些变化，本文是输入 $s,a$ 输出 $\hat q(s,a)$，原文是输入 $s$ 输出一系列 $\hat q(s,a_i)$，

# 9.策略梯度算法

第 $7$ 章到第 $8$​ 章的重要转变是求解的未知量从表格（离散）变为了函数（连续）上的，但之前的方法都是 value-based  本章及下一章都是 policy-based

value-based：以值为核心，对值进行优化，然后进行相似 PI 过程

policy-based：直接设计一个针对策略的函数去优化

本章内容先去找到一个 metrics 来度量策略的优劣，然后对这个 metrics 梯度上升最大化策略

之前的策略都是通过表格的形式存储的，现在改为函数：
$$
\pi(a|s,\theta),\theta \in \mathbb R^m
$$
输入一个 $s$ 就可以得到全部 $\pi(a|s,\theta)$，好处依然是减少存储，增强泛化能力，$\pi(a,s,\theta),\pi_\theta(a|s),\pi_\theta(a,s)$ 写法都是等价的，策略梯度的基本思想主要分为以下几点：

1. 找到一个 metrics 作为目标函数去定义最优策略：$J(\theta)$
2. 基于梯度去最大化目标函数

先解决问题 $1$，metrics 主要分为两类：

**1.average state value：**
$$
\bar v_\pi = \sum_{s \in \mathcal S}d(s)v_\pi(s)
$$
$d(s)$ 是对 $s$ 的权重，相当于 $v_\pi(s)$ 的加权平均，等价于 $\mathbb E[v_\pi(S)],S\sim d$，写作向量形式：
$$
\bar v_\pi = \sum_{s \in \mathcal S}d(s)v_\pi(s) = d^T v_\pi \\
v_\pi = [\dots,v_\pi(s),\dots]^T \in \mathbb R^{|\mathcal S|} \\
d = [\dots,d(s),\dots]^T \in \mathbb R^{|\mathcal S|}
$$
 对于 $d$ 的选取需要讨论，最简单的就是 $d$ 与 $\pi$ 无关，因为此时对其求梯度可以忽略 $d$，这种情况记 $d,\bar v_\pi$ 分别为 $d_0,\bar v_\pi^0$，对于 $d_0$ 的选取最简单的就是均匀分布，而有时会更关注某状态 $s_0$（一般是确定的初始状态），此时 $d_0(s_0) = 1,d_0(s \ne s_0) = 0$，此时 $\bar v_\pi^0 = v_\pi(s_0)$

而当 $d$ 与 $\pi$ 有关时是更常见的，此时令 $d = d_\pi(s)$，就是上一章提到的 stationary distribution，跳转无数次最后趋于稳定时的分布

**2.average one-step value：**
$$
\bar r_\pi = \sum_{s \in \mathcal S}d_\pi(s)r_{\pi}(s) = \mathbb E[r_\pi(S)],S \sim d_\pi \\
r_\pi(s) = \sum_{a \in \mathcal A}\pi(a|s)r(s,a) \\
r(s,a) = \mathbb E[R|s,a] = \sum_r rp(r|s,a)
$$
相当于希望求得即时奖励期望最大的策略，在论文中更常见的一种表述是根据策略采样了一个轨迹，对应奖励的随机变量为 $(R_{t+1},R_{t+2},\dots)$，对这条轨迹的 average one-step value 为：
$$
\begin{align*}
&\lim_{n \to \infty} \frac{1}{n} \mathbb{E} \Big[ R_{t+1} + R_{t+2} + \cdots + R_{t+n} \mid S_t = s_0 \Big] \\
= &\lim_{n \to \infty} \frac{1}{n} \mathbb{E} \Bigg[ \sum_{k=1}^{n} R_{t+k} \mid S_t = s_0 \Bigg]
\end{align*}
$$
$s_0$ 是起始状态，在跑了无穷多步之后起始状态就不重要了（非确定性）：
$$
\lim_{n \to \infty} \frac{1}{n} \mathbb{E} \Bigg[ \sum_{k=1}^{n} R_{t+k} \mid S_t = s_0 \Bigg] = \lim_{n \to \infty} \frac{1}{n} \mathbb{E} \Bigg[\sum^n_{k=1} R_{t+k} \Bigg] = \sum_s d_\pi(s)r_\pi(s)
$$
对于上述的 metrics 无论适合形式，都是 $\pi$ 的函数，$\theta$ 为参数，希望优化参数使 metrcis 最大化

对于 metrics 又分为 discounted case $\gamma \in[0,1)$ 和 undiscounted case $\gamma = 1$​，这里不多做讨论，均取 discounted case

比较 $\bar r_\pi$ 与 $\bar v_\pi$，直观上来看 $\bar r_\pi$ 比较近视只关心即时奖励，但其实它们是等价的，在 discounted case 情况下：
$$
\bar r_\pi = (1 - \gamma)\bar v_\pi
$$
原因是 $\bar v_\pi(\theta) = d^T_\pi v_\pi$ $\bar r_\pi(\theta) = d^T_\pi r_\pi$，又有 $v_\pi = r_\pi + \gamma P_\pi v_\pi$，故：
$$
\begin{align*}
d^T_\pi v_\pi &= d^T_\pi r_\pi + d^T_\pi \gamma P_\pi v_\pi \\
\bar v_\pi &= \bar r_\pi + \gamma (d^T_\pi P_\pi) v_\pi \\
\bar v_\pi &= \bar r_\pi + \gamma d^T_\pi v_\pi \\
\bar v_\pi &= \bar r_\pi + \gamma \bar v_\pi \\
\bar r_\pi &= (1 - \gamma)\bar v_\pi
\end{align*}
$$
在论文中还经常可以看到一种 metric：
$$
J(\theta) = \mathbb E \Bigg[ \sum_{t=0}^\infty \gamma^t R_{t+1} \Bigg]
$$
其实这个就是 $\bar v_\pi$：
$$
\begin{align*}
\mathbb E \Bigg[ \sum_{t=0}^\infty \gamma^t R_{t+1} \Bigg] &= \sum_s d(s) \mathbb E \Bigg[ \sum_{t=0}^\infty \gamma^t R_{t+1}\mid S_0 = s \Bigg] \\
&= \sum_s d(s) v(s) \\
&= \bar v_\pi
\end{align*}
$$
其中 $S \sim d$， $d$ 为任何分布都是等价的

问题 $1$ 解决完毕，下面考虑问题 $2$，求出 metrics 对应的梯度，也是最难的一部分

直接给出梯度：
$$
\nabla_\theta J(\theta) = \sum_{s \in \mathcal S}\eta(s)\sum_{a \in \mathcal A}\nabla_\theta \pi(a|s,\theta)q_\pi(s,a)
$$
先对 $s$ 求和，每个 $s$ 有一个权重 $\eta(s)$，其中 $J(\theta)$ 可以为 $\bar v_\pi,\bar r_\pi,\bar v^0_\pi$ 中任意一个，中间的 $=$ 可能为严格相等，也可能是近似，或者是成比例等于（差一个比例因子），$\eta$ 会在不同的情况下呈现不同的分布，一些特殊情况：
$$
\begin{align*}
\nabla_{\theta}\bar{r}_\pi &\simeq \sum_{s} d_\pi(s) \sum_{a} \nabla_{\theta}\pi(a|s, \theta) q_\pi(s, a), \\
\nabla_{\theta}\bar{v}_\pi &= \frac{1}{1-\gamma} \nabla_{\theta}\bar{r}_\pi \\
\nabla_{\theta}\bar{v}^0_\pi &= \sum_{s \in \mathcal{S}} \rho_\pi(s) \sum_{a \in \mathcal{A}} \nabla_{\theta}\pi(a|s, \theta) q_\pi(s, a)
\end{align*}
$$
对于$\bar r_\pi$ 在 discounted case 下是约等于，在 discounted case 下是严格等于

对于$\bar v_\pi$​ 在 discounted case 下是近似成比例，在 discounted case 下是差一个比例因子

对于$\bar v_\pi^0$，这里的 $\rho_\pi$ 是另一个分布这里不多说

**对梯度的获取的介绍说的很粗略，细节后续看书补充，现在就是知道可以统一写作这个形式**

对于梯度可以进行一些变形：
$$
\begin{align*}
\nabla_\theta J(\theta) &= \sum_{s \in \mathcal S}\eta(s)\sum_{a \in \mathcal A}\nabla_\theta \pi(a|s,\theta)q_\pi(s,a) \\
&= \mathbb E\Bigg[\nabla_\theta \ln \pi(a|S,\theta)q_\pi(S,A)\Bigg]
\end{align*}
$$
其中 $S \sim \eta,A \sim \pi(A|S,\theta)$，证明后面给出，先解释这样的好处，这是一个真实的梯度的期望，省去了求和，可以通过采样来对其估计：
$$
\nabla_\theta J \approx \nabla_\theta \ln \pi(a|s,\theta)q_\pi(s,a) \\
\nabla_\theta \ln \pi(a|s,\theta) = \frac{\nabla_\theta \pi(a|s,\theta)}{\pi(a|s,\theta)} \\
\nabla_\theta \pi(a|s,\theta) = \nabla_\theta \ln \pi(a|s,\theta)\pi(a|s,\theta)
$$
由上式可以证明：
$$
\begin{align*}
\nabla_\theta J(\theta) &= \sum_{s \in \mathcal S}\eta(s)\sum_{a \in \mathcal A}\nabla_\theta \pi(a|s,\theta)q_\pi(s,a) \\
&= \sum_{s \in \mathcal S}\eta(s)\sum_{a \in \mathcal A}\pi(a|s,\theta)\nabla_\theta \ln \pi(a|s,\theta)q_\pi(s,a) \\
&= \mathbb E_{S \sim d}\Bigg[\sum_{a \in \mathcal A}\pi(a|s,\theta)\nabla_\theta \ln \pi(a|S,\theta)q_\pi(S,a)\Bigg] \\
&= \mathbb E_{S \sim \eta,A \sim \pi}\Bigg[\nabla_\theta \ln \pi(A|S,\theta)q_\pi(S,A)\Bigg] \\
&= \mathbb E\Bigg[\nabla_\theta \ln \pi(A|S,\theta)q_\pi(S,A)\Bigg]
\end{align*}
$$
因为需要计算 $\ln\pi(a|s,\theta)$ 所以 $\pi(a|s,\theta) > 0,\forall s,a,\theta$，可以利用 softmax 将值映射到 $(0,1)$，同时也满足和为 $1$，即：
$$
\pi(a|s, \theta) = \frac{e^{h(s,a,\theta)}}{\sum_{a' \in \mathcal{A}} e^{h(s,a',\theta)}},
$$
其中 $h(s,a,\theta)$ 就是经过最后的 softmax 激活层之前的结果，最终得到的策略也是随机具有探索性的，对于 deterministic policy gradient（DPG） 方法可以舍去 $\pi(a|s,\theta) > 0,\forall s,a,\theta$ 这一限制，同时在动作空间很大时，该方法无效，而 DPG 依然有效

现在拥有了目标函数和梯度，下面要讨论的就是梯度上升的算法，第一个也是最简单的就是 REINFORCE，梯度算法的基本思想就是：
$$
\begin{align*}
\theta_{t+1} &= \theta_t + \alpha \nabla_{\theta} J(\theta) \\
&= \theta_t + \alpha \mathbb{E}\Big[ \nabla_{\theta} \ln \pi(A | S, \theta_t) q_\pi(S, A) \Big]
\end{align*}
$$
在 model-free 时是无法计算期望的，这里需要利用随机梯度来代替期望：
$$
\theta_{t+1} = \theta_t + \alpha  \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t) q_\pi(s_t, a_t)
$$
对于 $q_\pi(s_t, a_t)$，依然可以对其近似，最简单的就是 MC 的思想，从 $(s_t,a_t)$ 出发采样一个轨迹得到 $q_t(s_t,a_t)$ 来估计：
$$
\theta_{t+1} = \theta_t + \alpha  \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t) q_t(s_t, a_t)
$$
这就是 REINFORCE 算法，下面讨论如何对其采样，虽然 $S \sim d$，但是实际中一般不怎么考虑这一点，而 $A \sim \pi(A|S,\theta)$，所以在 $s_t$ 应该根据 $\pi(A|s_t,\theta)$ 采样，所以这是一个 on-policy 算法，两个策略都是 $\pi(\theta_t)$，也有 off-policy 算法，有一些额外技巧，下一章说明

根据 $\nabla_\theta \pi(a|s,\theta) = \nabla_\theta \ln \pi(a|s,\theta)\pi(a|s,\theta)$ 重写算法：
$$
\begin{align*}
\theta_{t+1} &= \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t) q_t(s_t, a_t) \\
&= \theta_t + \alpha \underbrace{\Biggl(\frac{q_t(s_t, a_t)}{\pi(a_t | s_t, \theta_t)}\Biggl)}_{\beta_t} \nabla_{\theta} \pi(a_t | s_t, \theta_t) \\
&= \theta_t + \alpha {\beta_t} \nabla_{\theta} \pi(a_t | s_t, \theta_t)
\end{align*}
$$
$\alpha {\beta_t}$ 成为新的步长，相当于在优化 $\pi(a_t | s_t, \theta)$，当步长比较小的时候有一些性质（因为这个时候沿着梯度走才确保上升）：

当 $\beta_t > 0$ 时，$\pi(a_t | s_t, \theta_{t+1}) > \pi(a_t | s_t, \theta_t)$

当 $\beta_t < 0$ 时，$\pi(a_t | s_t, \theta_{t+1}) < \pi(a_t | s_t, \theta_t)$

很直观，也可以利用一阶展开或中值定理证明，当 $||\theta_{t+1} - \theta_t||$ 足够小时有：
$$
\begin{align*}
\pi(a_t | s_t, \theta_{t+1}) &\approx \pi(a_t | s_t, \theta_t) + (\nabla_{\theta} \pi(a_t | s_t, \theta_t))^T (\theta_{t+1} - \theta_t) \\
&= \pi(a_t | s_t, \theta_t) + \alpha \beta_t (\nabla_{\theta} \pi(a_t | s_t, \theta_t))^T (\nabla_{\theta} \pi(a_t | s_t, \theta_t)) \\
&= \pi(a_t | s_t, \theta_t) + \alpha \beta_t \|\nabla_{\theta} \pi(a_t | s_t, \theta_t)\|^2
\end{align*}
$$
 $\beta_t$ 可以很好的在 exploration 和 exploitation 之间平衡（探索与剥削/充分利用）：
当 $q_t(s_t,a_t)$ 越大时，$\beta_t$​ 就越大，向着该方向更新更大（exploitation）

当 $\pi(a_t|s_t,\theta_t)$ 越小时，$\beta_t$ 越大，向着该方向更新更大（exploration）

REINFORCE 算法流程：

![image-20250825034920885](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250825034920885.png)

可见是 on-policy 算法，并且是 offline 的

# 10.Actor-Critic 方法

目前 RL 最流行的方法之一，它本身也是一种策略梯度算法，不过它同时将值函数估计引入到了其中，简称 AC

actor 指的是策略更新，策略是用来采取行动的，所以 take action 的东西就对应着 actor

crtic 指的是 PE 或者 value estimation，中文是批评家，对策略进行评估

对于上节课的算法：
$$
\theta_{t+1} = \theta_t + \alpha  \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t) q_t(s_t, a_t)
$$
这个算法就是对策略的更新，就是 actor，而 $q_t(s,a)$​ 的获取方法就是 critic，在上节课中介绍到是 MC 的思想，也就是完整的采样一个 episode 计算 return，此时整个算法就称为 REINFORCE 或者 Monte Carlo policy gradient，上一章介绍过了

用 TD 方法去估计 $q_t(s,a)$，此时这个算法称为 actor-critic：

![image-20250825163233477](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250825163233477.png)

相当于 crtic 对 $q_t(s,a)$ 的获取使用的是结合值函数近似的 Sarsa 算法，$q$ 是一个神经网络，输入 $(s,a)$ 获得对应的 action value

这里的 actor 对应的是 policy update algorithm，crtic 对应的 Sarsa + value function approximation，显然这个是 on-policy 的，该算法通常称为 QAC，Q 指的是 q value，揭示了 AC 的本质思想

## advantage actor-critic（A2C）

advantage actor-critic（A2C）是 QAC 的一个推广，核心思想是在 QAC 中引入一个偏置量减少估计的方差

对于偏置 $b(S)$ 是一个 $S$ 的标量函数，无论 $b$ 表达式如何，都有：
$$
\begin{align*}
\nabla_\theta J(\theta_t) &= \mathbb E_{S \sim \eta,A \sim \pi}\Bigg[\nabla_\theta \ln \pi(A|S,\theta_t)q_\pi(S,A)\Bigg] \\
&= \mathbb E_{S \sim \eta,A \sim \pi}\Bigg[\nabla_\theta \ln \pi(A|S,\theta_t)[q_\pi(S,A) - b(S)]\Bigg]
\end{align*}
$$
证明这个式子成立就等价于证明后一项期望为 $0$：
$$
\begin{align*}
\nabla_\theta J(\theta_t) &= \mathbb E_{S \sim \eta,A \sim \pi}\Bigg[\nabla_\theta \ln \pi(A|S,\theta_t)b(S)\Bigg] \\
&= \sum_{s \in \mathcal S}\eta(s) \sum_{a \in \mathcal A} \pi(a|s,\theta_t) \nabla_\theta \ln \pi(a|s,\theta_t)b(s) \\
&= \sum_{s \in \mathcal S}\eta(s) \sum_{a \in \mathcal A} \nabla_\theta \pi(a|s,\theta_t)b(s) \\
&= \sum_{s \in \mathcal S}\eta(s)b(s) \sum_{a \in \mathcal A} \nabla_\theta \pi(a|s,\theta_t) \quad\text{这一步可以清晰的看出 b 的作用}\\
&= \sum_{s \in \mathcal S}\eta(s)b(s) \nabla_\theta \sum_{a \in \mathcal A} \pi(a|s,\theta_t) \quad\text{后一项求和为 }1\\
&= 0
\end{align*}
$$
所以引入 $b(S)$ 不会影响梯度下降，下面解释它的作用，假设 $\nabla_\theta J(\theta) = \mathbb E[X]$ 其中：
$$
X(S,A) = \nabla_\theta \ln \pi(A|S,\theta_t)[q(S,A) - b(S)]
$$
上述证明 $b(S)$ 不影响 $\mathbb E[X]$，但是会影响 $var(X)$，因为：
$$
tr[var(X)] = \mathbb E[X^T X] - \bar x^T \bar x
$$
后者保持不变，对于前者：
$$
\begin{align*}
\mathbb{E}[X^T X] &= \mathbb{E}\big[(\nabla_{\theta} \ln \pi)^T (\nabla_{\theta} \ln \pi)(q(S,A) - b(S))^2\big] \\
&= \mathbb{E}\big[\|\nabla_{\theta} \ln \pi\bigr|^2(q(S,A) - b(S))^2\big]
\end{align*}
$$
显然 $b$ 不同时，方差也是有明显变化的，可以找到最优 baseline $b$ 使得方差最小，这样的好处是在采样时可以拥有更小的误差，对于之前的算法可以看作是 $b(S) = 0$ 的，并不是一个很好的 baseline，给出 optimal baseline 的结论：
$$
b^*(s) = \frac{\mathbb{E}_{A \sim \pi} \left[ \left\| \nabla_{\theta} \ln \pi(A | s, \theta_t) \right\|^2 q(s, A)\right]}{\mathbb{E}_{A \sim \pi} \left[ \left\| \nabla_{\theta} \ln \pi(A | s, \theta_t) \right\|^2\right]}.
$$
证明后续补，虽然是最优的，但是太复杂了，$\left\| \nabla_{\theta} \ln \pi(A | s, \theta_t) \right\|^2$ 相当于权重，将其忽略：
$$
b(s) = \mathbb{E}_{A \sim \pi} \left[q(s, A)\right] = v_\pi(s)
$$
很多文献上说其为最优的，但其实这是简化过的版本，下面引入到算法中：
$$
\begin{align*}
\theta_{t+1} &= \theta_t + \alpha \mathbb{E} \Big[ \nabla_{\theta} \ln \pi(A|S, \theta_t) [q_\pi(S, A) - v_\pi(S)] \\
&= \theta_t + \alpha \mathbb{E} \Big[ \nabla_{\theta} \ln \pi(A|S, \theta_t) \delta_\pi(S, A) \Big]
\end{align*}
$$
其中新定义的 $\delta_\pi$ 称为 advantage function，通常用 $A$ 表示，这里为与动作区分，叫优势函数原因是这是 action value 与 state value 的差，如果越大则代表这是当前状态一个比较好的动作，于是迭代算法为：
$$
\begin{align*}
\theta_{t+1} &= \theta_t + \alpha  \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t)[q_t(s_t,a_t) - v_t(s_t)] \\
&= \theta_t + \alpha  \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t)\delta_t(s_t,a_t) \\
&= \theta_t + \alpha \underbrace{\Biggl(\frac{\delta_t(s_t, a_t)}{\pi(a_t | s_t, \theta_t)}\Biggl)}_{\text{step size}} \nabla_{\theta} \pi(a_t | s_t, \theta_t)
\end{align*}
$$
$\delta_t$ 显然比 $q_t$ 好，因为关键是相对值而不是绝对值，更好的平衡了充分利用和探索，对于 $\delta_t$ 利用 TD 的思想代入为 TD error：
$$
\delta_t = q_t(s_t,a_t) - v_t(s_t) \to r_{t+1} + \gamma v_t(s_{t+1}) -v_t(s_t)
$$
原因是：
$$
\mathbb{E}[q_\pi(S,A) - v_\pi(S) | S = s_t, A = a_t] = \mathbb{E}\Big[R + \gamma v_\pi(S') - v_\pi(S) | S = s_t, A = a_t\Big]
$$
好处是原来的式子需要一个神经网络近似 $q_t$ 一个近似 $v_t$ 现在只需要一个近似 $v_t$ 了，流程：

![image-20250825175508111](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250825175508111.png)

前两步就是 TD 求 state value 只是显示的先求出 $\delta$ 在 actor 中还要用，on-policy

之前的算法都是 on-policy 的，转化为 off-policy 后可以复用之前的一些经验，下面介绍 off-policy 的算法，其实是在 on-policy 上修改，这个技术较 importance sampling（重要性采样），不只局限于 AC，任何一个求期望的算法都可以使用

对于用采样去估计期望已经很熟悉了，但这个的前提是可以在真实的分布中去采样，而如果只能在另一个分布上去采样的话，得到的结果也是在另一个分布下的期望值，该方法就无效了

现在的问题就是有一些数据 $\{x_i\} \sim p_1$，如何求得 $\mathbb E_{X\sim p_0}[X]$？如果解决了这个问题就可以进行 off-policy 了，这样就可以使用对于 behavior policy $\beta$ 采样得到的样本用于求 target policy $\pi$ 下的期望 $\mathbb E_{A \sim \pi}[*]$，注意：
$$
\mathbb{E}_{X \sim p_0}[X] = \sum_{x} p_0(x)x = \sum_{x} p_1(x) \underbrace{\frac{p_0(x)}{p_1(x)}x}_{f(x)} = \mathbb{E}_{X \sim p_1}[f(X)]
$$
这就是重要性采样的核心思想，下面去估计 $\mathbb{E}_{X \sim p_1}[f(X)]$，对于 $x_i \sim p_1$：
$$
\bar f = \frac{1}{n}\sum_{i=1}^n f(x_i) \\
\mathbb{E}_{X \sim p_1}[\bar f] = \mathbb{E}_{X \sim p_1}[f(X)] \\
\text{var}_{X \sim p_1}[\bar f] = \frac{1}{n}\text{var}_{X \sim p_1}[f(X)]
$$
所以：
$$
\mathbb{E}_{X \sim p_0}[X] \approx \bar f = \frac{1}{n}\sum_{i=1}^n f(x_i) = \frac{1}{n}\sum_{i=1}^n \frac{p_0(x_i)}{p_1(x_i)}x_i
$$
**$\frac{p_0(x_i)}{p_1(x_i)}$ 称为 importance weight，相当于在 $p_1$ 下去调整权重，注意这个式子中出现了 $p_0$，刚看到可能疑惑了如果知道 $p_0$ 不就可以直接求期望了吗，第一种情况就是 $x$ 是连续的，此时如果 $p_0$ 表达式很复杂是求不出积分的，更重要的一种情况是我中的 $p_0(x)$ 但是是不知道解析式的，如神经网络中，输入 $x$ 就可以知道对应的概率，如果想直接求解需要把所有情况都穷举出，这也是不现实的，这就是这个算法的意义（显然如果不知道 $p_0(x)$ 什么算法也不可能去估计在 $p_0$ 下的期望的）**

下面尝试将这一方法融入 AC 中，现在的任务是有一个 behavior policy $\beta$ 并采样得到了一系列样本，要用这些经验去最大化：
$$
J(\theta) = \sum_{s \in \mathcal S}d_\beta(s)v_\pi(s) = \mathbb E_{S \sim d_\beta}[v_\pi(S)]
$$
它的梯度为（证明后续补充）：
$$
\nabla_{\theta} J(\theta) = \mathbb{E}_{S \sim \rho, A \sim \beta} \left[ \frac{\pi(A | S, \theta)}{\beta(A | S)} \nabla_{\theta} \ln \pi(A | S, \theta) q_\pi(S, A) \right]
$$
根据重要性采样就等价于：
$$
\mathbb{E}_{S \sim \rho, A \sim \pi} \left[\nabla_{\theta} \ln \pi(A | S, \theta) q_\pi(S, A) \right]
$$
也就是根据此目标函数也可以向着最优策略去收敛，这里依然可以加入 $b(S)$，常见的就是 $b(S) = v_\pi(S)$，依然使用随机梯度 + TD 的思想去估计期望：
$$
\theta_{t+1} = \theta_t + \alpha_\theta \frac{\pi(a_t | s_t, \theta_t)}{\beta(a_t | s_t)} \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t) (q_t(s_t, a_t) - v_t(s_t))
$$
依然是根据 A2C 中用 $\delta_t(s_t,a_t) = r_{t+1} + \gamma v_t(s_{t+1}) - v_t(s_t)$ 来近似 $q_t(s_t,a_t) - v_t(s_t)$：
$$
\theta_{t+1} = \theta_t + \alpha_\theta \frac{\pi(a_t | s_t, \theta_t)}{\beta(a_t | s_t)} \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t)\delta_t(s_t,a_t)
$$
化简得到：
$$
\theta_{t+1} = \theta_t + \alpha_\theta \frac{\delta_t(s_t,a_t)}{\beta(a_t | s_t)} \nabla_{\theta} \pi(a_t | s_t, \theta_t)
$$
对于这个 step-size 依然有同样的分析，流程：

![image-20250825192445352](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250825192445352.png)

之前介绍（$9,10$ 章）的都要求 $\pi(a|s) > 0$ 是随机策略，下面介绍的是 deterministic actor-critic（DPG）

提出这个的原因也解释过，如果动作空间是连续的，之前的方法都无效了，如果策略为确定性是一个前提，就可以舍弃 $\pi$ 记号而是：a
$$
a = \mu(s,\theta) = \mu(s)
$$
$\mu$ 是 $\mathcal S \to \mathcal A$ 的映射，之前的目标函数 $J$ 都是要求是随机策略的，而这里强制为确定性策略,，相当于对确定性策略定义了新的 metric：
$$
J(\theta) = \mathbb E[v_\mu(s)] = \sum_{s \in \mathcal S}d_0(s)v_\mu(s)
$$
$d_0$ 就是一种概率分布，与 $\mu$ 无关，有两种特殊的取法：

1. 只关注某个状态则 $d_0(s_0) = 1,d_0(s \ne s_0) = 0$
2. $d_0$ 为 behaviour policy $\beta$ 的分布

依然是直接给出梯度，天然的符合 off-policy：
$$
\begin{align*}
\nabla_{\theta} J(\theta) &= \sum_{s \in \mathcal{S}} \rho_\mu(s) \nabla_{\theta} \mu(s) \bigl(\nabla_a q_\mu(s, a)\big)\big|_{a=\mu(s)} \\
&= \mathbb{E}_{S \sim \rho_\mu} \big[ \nabla_{\theta} \mu(S) \big(\nabla_a q_\mu(S, a)\big)\big|_{a=\mu(S)} \big]
\end{align*}
$$
目前不需要，后续再补























